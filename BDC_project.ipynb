{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install plotly\n",
    "!pip install chart-studio\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work With Files\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "# Useful libraries:\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sin, cos, atan2, asin, sqrt, radians\n",
    "\n",
    "# To Plot:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Pyspark Lib:\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Preprocess:\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# Pysparl ML:\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_BUCKET = 'gs://nyc_comp_bk/'\n",
    "PATH_DATA = '/home/ubuntu/NYC_Taxi/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/ubuntu/NYC_Taxi/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Work_On_Bucket():\n",
    "    \n",
    "    def __init__(self, bucket_name):\n",
    "        # Get access to the bucket:\n",
    "        storage_client = storage.Client()\n",
    "        self.bucket = storage_client.get_bucket(bucket_name)\n",
    "        \n",
    "    def get_file_from_bucket(self, file_name, save_path):\n",
    "        # Download the file:\n",
    "        blob = self.bucket.blob(file_name)\n",
    "        blob.download_to_filename(''.join([save_path, file_name]))\n",
    "            \n",
    "    def upload_file_to_bucket(self, file_name, folder_path):\n",
    "        # Upload the File\n",
    "        object_to_save = self.bucket.blob(file_name)\n",
    "        object_to_save.upload_from_filename(folder_path + file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bucket = Work_On_Bucket('nyc_comp_bk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set kaggle:\n",
    "! mkdir ~/.kaggle\n",
    "Bucket.get_file_from_bucket('kaggle.json', '/home/ubuntu/NYC_Taxi/')\n",
    "! cp /home/ubuntu/NYC_Taxi/kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
      "Downloading new-york-city-taxi-fare-prediction.zip to /home/ubuntu/NYC_Taxi\n",
      " 99%|█████████████████████████████████████▋| 1.55G/1.56G [00:25<00:00, 56.1MB/s]\n",
      "100%|██████████████████████████████████████| 1.56G/1.56G [00:25<00:00, 66.2MB/s]\n",
      "Archive:  new-york-city-taxi-fare-prediction.zip\n",
      "  inflating: /home/ubuntu/NYC_Taxi/data/GCP-Coupons-Instructions.rtf  \n",
      "  inflating: /home/ubuntu/NYC_Taxi/data/sample_submission.csv  \n",
      "  inflating: /home/ubuntu/NYC_Taxi/data/test.csv  \n",
      "  inflating: /home/ubuntu/NYC_Taxi/data/train.csv  \n"
     ]
    }
   ],
   "source": [
    "# Download The Dataset\n",
    "!kaggle competitions download -c new-york-city-taxi-fare-prediction\n",
    "\n",
    "# Unzip the Files\n",
    "! unzip new-york-city-taxi-fare-prediction.zip -d /home/ubuntu/NYC_Taxi/data/\n",
    "! rm new-york-city-taxi-fare-prediction.zip\n",
    "\n",
    "# Upload databses to bucket:\n",
    "print('Start Uploding!')\n",
    "Bucket.upload_file_to_bucket('train.csv', PATH_DATA)\n",
    "Bucket.upload_file_to_bucket('test.csv', PATH_DATA)\n",
    "print('Succesfully Uploaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Steps (Load + Checks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data:\n",
    "train = spark.read.load(PATH_BUCKET+\"train.csv\", format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "test = spark.read.load(PATH_BUCKET+\"test.csv\", format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# Load Test (Because pyspark changes the timestamp):\n",
    "Bucket.get_file_from_bucket('test.csv', '')\n",
    "original_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is 55423856 rows by 8 columns\n"
     ]
    }
   ],
   "source": [
    "# Get DB shape:\n",
    "ncol = len(train.columns)\n",
    "nrow = train.count()\n",
    "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(nrow, ncol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: timestamp (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the schema:\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>55423856</td>\n",
       "      <td>55423856</td>\n",
       "      <td>55423856</td>\n",
       "      <td>55423856</td>\n",
       "      <td>55423480</td>\n",
       "      <td>55423480</td>\n",
       "      <td>55423856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>11.345045601663852</td>\n",
       "      <td>None</td>\n",
       "      <td>-72.50968444358728</td>\n",
       "      <td>39.919791786888176</td>\n",
       "      <td>-72.5112097297181</td>\n",
       "      <td>39.92068144482884</td>\n",
       "      <td>1.6853799201556816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>20.7108321982325</td>\n",
       "      <td>None</td>\n",
       "      <td>12.848883381402652</td>\n",
       "      <td>9.642353041994932</td>\n",
       "      <td>12.782196517830771</td>\n",
       "      <td>9.633345796415126</td>\n",
       "      <td>1.327664357095968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>2009-01-01 00:00:27 UTC</td>\n",
       "      <td>-3442.059565</td>\n",
       "      <td>-3492.263768</td>\n",
       "      <td>-3442.024565</td>\n",
       "      <td>-3547.886698</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>93963.36</td>\n",
       "      <td>2015-06-30 23:59:54 UTC</td>\n",
       "      <td>3457.625683</td>\n",
       "      <td>3408.789565</td>\n",
       "      <td>3457.62235</td>\n",
       "      <td>3537.132528</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary         fare_amount          pickup_datetime    pickup_longitude  \\\n",
       "0   count            55423856                 55423856            55423856   \n",
       "1    mean  11.345045601663852                     None  -72.50968444358728   \n",
       "2  stddev    20.7108321982325                     None  12.848883381402652   \n",
       "3     min              -300.0  2009-01-01 00:00:27 UTC        -3442.059565   \n",
       "4     max            93963.36  2015-06-30 23:59:54 UTC         3457.625683   \n",
       "\n",
       "      pickup_latitude   dropoff_longitude   dropoff_latitude  \\\n",
       "0            55423856            55423480           55423480   \n",
       "1  39.919791786888176   -72.5112097297181  39.92068144482884   \n",
       "2   9.642353041994932  12.782196517830771  9.633345796415126   \n",
       "3        -3492.263768        -3442.024565       -3547.886698   \n",
       "4         3408.789565          3457.62235        3537.132528   \n",
       "\n",
       "      passenger_count  \n",
       "0            55423856  \n",
       "1  1.6853799201556816  \n",
       "2   1.327664357095968  \n",
       "3                   0  \n",
       "4                 208  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show some basic Statistics:\n",
    "stats = train.select(train.columns[1:]).describe()\n",
    "stats.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_datetime: 0\n",
      "pickup_longitude: 0\n",
      "pickup_latitude: 0\n",
      "dropoff_longitude: 376\n",
      "dropoff_latitude: 376\n",
      "passenger_count: 0\n"
     ]
    }
   ],
   "source": [
    "# Check Nulls:\n",
    "for c in train.columns[2:]:\n",
    "    nans = train.where(col(c).isNull()).count()\n",
    "    print('{:s}: {:d}'.format(c, nans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Rows with Missing Values:\n",
    "train = train.na.drop(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duplicates are: 1650\n"
     ]
    }
   ],
   "source": [
    "# Check Duplicates:\n",
    "print('The Duplicates are: {:d}'.format(train.count()-train.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates:\n",
    "train = train.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work With the Fare Price (Dependent Variale):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the statistics, it is clear that some values are not possible, I am going to remove negatives values and all values that are lower that 2.50 dollars (https://nymag.com/nymetro/urban/features/taxi/n_20286/). Hence, I will keep all values greather than 2.50 dollars. By taking into account the max value, it is clear that some outlayers are present, nobady would want to spend 93,963 dollars for a trip. Working with the quantiles it is clear that there is somenthing wrong with the fare amount, the median is 999.9 dollars. As starting point let's remove what I am pretty sure that is an outlayer, namely everything over the fourth quntile, which represents all values greater than 93963.36 dollars. Computing again the quantiles now the fourth quantile is 999.99 dollars, Checking the other variables associated with these high fare amounts, it comes up that some of them doesn't make any sense: no changes in lat and long pick up and drop off, too small journey with one or two people for a so high price, missing values for lat and long and so on. I am going to keep al observations with fear amount less than 999.99 dollars, hopping that with the EDA I can clean more the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>55421830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>11.345105672079605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>20.711131158405834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>-300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>93963.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary         fare_amount\n",
       "0   count            55421830\n",
       "1    mean  11.345105672079605\n",
       "2  stddev  20.711131158405834\n",
       "3     min              -300.0\n",
       "4     max            93963.36"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get statistics:\n",
    "train.select('fare_amount').describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Values lower than 2.50$:\n",
    "train = train.filter('fare_amount > 2.50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.51, 400.0, 93963.36, 93963.36]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Quantiles:\n",
    "train.approxQuantile('fare_amount', [0.25, 0.50, 0.75, 0.975], 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Outlayers:\n",
    "train = train.filter('fare_amount < 93963.36')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.51, 419.75, 75747.02, 75747.02]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Again Quantiles\n",
    "train.approxQuantile('fare_amount', [0.25, 0.50, 0.75, 0.975], 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove other Outlayers:\n",
    "train = train.filter('fare_amount < 999.99')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.51, 400.0, 978.0, 978.0]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Quantiles:\n",
    "train.approxQuantile('fare_amount', [0.25, 0.50, 0.75, 0.975], 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create My Base Line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Base Line I am going to use a Multiple Linear Regression that takes as input all the scaled (mean=0, sd=1) numerical variables.\n",
    "As result I get an RMSE = 9.40719 on the Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL = ['pickup_longitude',\n",
    "            'pickup_latitude',\n",
    "            'dropoff_longitude',\n",
    "            'dropoff_latitude',\n",
    "            'passenger_count']\n",
    "TARGET = 'fare_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature Vector:\n",
    "assembler = VectorAssembler(inputCols=NUMERICAL, outputCol=\"features\")\n",
    "train_df = assembler.transform(train)\n",
    "train_df = train_df.select('features', TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Data:\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"ScaledFeatures\")\n",
    "scalerModel = scaler.fit(train_df)\n",
    "train_df = scalerModel.transform(train_df)\n",
    "train_df = train_df.select('ScaledFeatures', TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Training Set *****\n",
      "RMSE: 9.792\n",
      "MAE: 6.030\n",
      "R2: 0.000\n",
      "***** Training Set *****\n"
     ]
    }
   ],
   "source": [
    "# Run the Linear Regression:\n",
    "lr = LinearRegression(featuresCol=\"ScaledFeatures\", labelCol=TARGET, maxIter=10)\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Print Stats:\n",
    "training_result = lr_model.summary\n",
    "print(\"***** Training Set *****\")\n",
    "print(\"RMSE: {:.3f}\".format(training_result.rootMeanSquaredError))\n",
    "print(\"MAE: {:.3f}\".format(training_result.meanAbsoluteError))\n",
    "print(\"R2: {:.3f}\".format(training_result.r2))\n",
    "print(\"***** Training Set *****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 459k/459k [00:02<00:00, 228kB/s]\n",
      "Successfully submitted to New York City Taxi Fare PredictionfileName        date                 description       status    publicScore  privateScore  \n",
      "--------------  -------------------  ----------------  --------  -----------  ------------  \n",
      "submission.csv  2021-06-26 11:42:04  First Submission  complete  9.40719      9.40719       \n",
      "submission.csv  2021-06-23 15:15:57  First Submission  complete  9.40712      9.40712       \n",
      "submission.csv  2021-06-23 15:10:56  First Submission  complete  9.40712      9.40712       \n",
      "submission.csv  2021-06-23 14:58:58  None              error     None         None          \n",
      "submission.csv  2021-06-23 14:58:06  First Submission  error     None         None          \n",
      "submission.csv  2021-06-23 14:55:46  None              error     None         None          \n",
      "submission.csv  2021-06-23 14:54:33  First Submission  error     None         None          \n",
      "submission.csv  2021-06-23 14:52:36  None              error     None         None          \n",
      "submission.csv  2021-06-23 14:51:55  First Submission  error     None         None          \n",
      "submission.csv  2021-06-23 14:51:27  First Submission  error     None         None          \n",
      "submission.csv  2021-06-23 14:49:09  None              error     None         None          \n",
      "submission.csv  2021-06-23 14:47:35  First Submission  error     None         None          \n"
     ]
    }
   ],
   "source": [
    "# Prepare the test:\n",
    "test_df = assembler.transform(test.select(NUMERICAL))\n",
    "test_df = test_df.select('features')\n",
    "test_df = scalerModel.transform(test_df)\n",
    "\n",
    "# Make Predictions:\n",
    "predictions = lr_model.transform(test_df).select('prediction').withColumnRenamed('prediction','fare_amount').toPandas()\n",
    "\n",
    "# Prepare the Submission:\n",
    "submission = pd.concat([original_test['key'], predictions['fare_amount']], axis=1)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Submit:\n",
    "!kaggle competitions submit -c new-york-city-taxi-fare-prediction -f submission.csv -m \"First Submission\"\n",
    "!kaggle competitions submissions -c new-york-city-taxi-fare-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Latitude and Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the study of the quantiles it is clear that is plenty of outlayers, because the NYC coordinates are (lat=40.730610 lon=-73.935242). As first step, it is possible to remove everything that is out of the I and III which are very extrem values (as we can see). \n",
    "\n",
    "The it is possible to compute new features that can be useful both for the model and to remove more outlayers, namely the Absolute Distance, the Haversinee Distance and the Minkowski Distance (that for p=1 is the Manhattan and for p=2 is the Euclidean distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3442.059565,  2084.697707,  3457.625683],\n",
       "       [-3492.263768,  1963.515858,  3408.789565],\n",
       "       [-3442.024565,   -73.979903,  3457.62235 ],\n",
       "       [-3547.886698,  3015.889707,  3537.132528]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check I, II and III Quantiles:\n",
    "np.array([train.approxQuantile('pickup_longitude', [0.25, 0.50, 0.75], 0.25),\n",
    "          train.approxQuantile('pickup_latitude', [0.25, 0.50, 0.75], 0.25),\n",
    "          train.approxQuantile('dropoff_longitude', [0.25, 0.50, 0.75], 0.25),\n",
    "          train.approxQuantile('dropoff_latitude', [0.25, 0.50, 0.75], 0.25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any observation out of the I and III quantile, for each lat/long variable:\n",
    "train = train.filter((col('pickup_longitude') > -3442.059565) |\n",
    "             (col('pickup_longitude') < 3457.625683) |\n",
    "             (col('pickup_latitude') > -3492.263768) |\n",
    "             (col('pickup_latitude') < 3408.789565) |\n",
    "             (col('dropoff_longitude') > -3442.024565) |\n",
    "             (col('dropoff_longitude') < 3457.62235) |\n",
    "             (col('dropoff_latitude')  >-3547.886698) |\n",
    "             (col('dropoff_latitude') < 3537.1325288)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-----------------------+----------------+---------------+-----------------+----------------+---------------+-------------------------+\n",
      "|key                |fare_amount|pickup_datetime        |pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|pickup_abs_dist_longitude|\n",
      "+-------------------+-----------+-----------------------+----------------+---------------+-----------------+----------------+---------------+-------------------------+\n",
      "|2010-08-28 06:46:14|5.3        |2010-08-28 06:46:14 UTC|-73.99588       |40.725325      |-74.008352       |40.715012       |1              |0.012472000000002481     |\n",
      "+-------------------+-----------+-----------------------+----------------+---------------+-----------------+----------------+---------------+-------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write the function to compute the distances:\n",
    "\n",
    "def haversine(pick_lat, drop_lat, pick_long, drop_long):\n",
    "    R = 6371000\n",
    "    phi_1 = radians(pick_lat)    \n",
    "    phi_2 = radians(drop_lat) \n",
    "    delta_phi = radians(drop_lat - pick_lat) \n",
    "    delta_lambda = radians(drop_lon - pick_lon) \n",
    "    \n",
    "    a = sin(delta_phi / 2.0) ** 2 + cos(phi_1) * cos(phi_2) * sin(delta_lambda / 2.0) ** 2  \n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    meters = R * c \n",
    "    \n",
    "    return meters\n",
    "    \n",
    "def get_distance(name, p, df):\n",
    "    if name == 'ABS':\n",
    "        df.withColumn( 'pickup_abs_dist_longitude', ( abs(df['pickup_longitude'] - df['dropoff_longitude']) ) )\n",
    "        df.withColumn( 'pickup_abs_dist_latitude', ( abs(df['pickup_latitude'] - df['dropoff_latitude']) ) )\n",
    "        return df\n",
    "    elif name == 'Haversine':\n",
    "        R = 6371000\n",
    "  \n",
    "            \n",
    "        a = math.sin(delta_phi / 2.0) ** 2 + math.cos(phi_1) * math.cos(phi_2) * math.sin(delta_lambda / 2.0) ** 2        \n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))   \n",
    "        meters = R * c  \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(pick_lat, drop_lat, pick_long, drop_long):\n",
    "    R = 6371000\n",
    "    phi_1 = radians(pick_lat)    \n",
    "    phi_2 = radians(drop_lat) \n",
    "    delta_phi = radians(drop_lat - pick_lat) \n",
    "    delta_lambda = radians(drop_lon - pick_lon) \n",
    "    \n",
    "    a = sin(delta_phi / 2.0) ** 2 + cos(phi_1) * cos(phi_2) * sin(delta_lambda / 2.0) ** 2  \n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    meters = R * c \n",
    "    \n",
    "    return meters\n",
    "\n",
    "harve_col = udf(haversine, DoubleType())\n",
    "train.withColumn( 'harvesine', harve_col('pickup_latitude', 'dropoff_latitude', 'pickup_longitude', 'dropoff_longitude') ).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "modo = train.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "|                key|fare_amount|     pickup_datetime|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|\n",
      "+-------------------+-----------+--------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "|2009-07-26 23:46:00|        9.3|2009-07-26 23:46:...|      -74.008575|      40.720215|       -73.985928|       40.758278|              2|\n",
      "|2009-12-10 15:19:00|        7.3|2009-12-10 15:19:...|       -74.00043|      40.726082|        -73.98618|       40.754742|              3|\n",
      "+-------------------+-----------+--------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modo.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(latit_a, longit_a, latit_b, longit_b):\n",
    "    longit_a, latit_a, longit_b, latit_b = map(radians, [longit_a,  latit_a, longit_b, latit_b])\n",
    "    dist_longit = longit_b - longit_a\n",
    "    dist_latit = latit_b - latit_a\n",
    "    # Calculate area\n",
    "    area = sin(dist_latit/2)**2 + cos(latit_a) * cos(latit_b) * sin(dist_longit/2)**2\n",
    "    # Calculate the central angle\n",
    "    central_angle = 2 * asin(sqrt(area))\n",
    "    radius = 6371\n",
    "    # Calculate Distance\n",
    "    distance = central_angle * radius\n",
    "    return abs(round(distance, 2))\n",
    "udf_get_distance = udf(get_distance, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o698.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 4 times, most recent failure: Lost task 0.3 in stage 43.0 (TID 2821, pyspark-w-2.us-central1-a.c.nyc-taxi-317516.internal, executor 93): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 155, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 76, in <lambda>\n    return lambda *a: f(*a)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-61-e79d0a87af83>\", line 2, in get_distance\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 42, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:336)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:290)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3284)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2495)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2495)\n\tat org.apache.spark.sql.Dataset$$anonfun$51.apply(Dataset.scala:3265)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3264)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2495)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2709)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 155, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 76, in <lambda>\n    return lambda *a: f(*a)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-61-e79d0a87af83>\", line 2, in get_distance\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 42, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:336)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:290)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-adb203524df2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'harvesine'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudf_get_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pickup_latitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dropoff_longitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pickup_latitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dropoff_longitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o698.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 4 times, most recent failure: Lost task 0.3 in stage 43.0 (TID 2821, pyspark-w-2.us-central1-a.c.nyc-taxi-317516.internal, executor 93): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 155, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 76, in <lambda>\n    return lambda *a: f(*a)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-61-e79d0a87af83>\", line 2, in get_distance\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 42, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:336)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:290)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3284)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2495)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2495)\n\tat org.apache.spark.sql.Dataset$$anonfun$51.apply(Dataset.scala:3265)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3264)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2495)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2709)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 155, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 76, in <lambda>\n    return lambda *a: f(*a)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-61-e79d0a87af83>\", line 2, in get_distance\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 42, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:336)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:290)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "modo.withColumn( 'harvesine', udf_get_distance(modo['pickup_latitude'], modo['dropoff_longitude'], modo['pickup_latitude'], modo['dropoff_longitude']) ).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
