{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work With Files\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "# Useful libraries:\n",
    "from __future__ import division\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "# To Plot:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pyspark Lib:\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Preprocess:\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler, StandardScaler\n",
    "\n",
    "# Pysparl ML:\n",
    "from pyspark.ml.tuning import CrossValidator, TrainValidationSplit, ParamGridBuilder\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_BUCKET = 'gs://nyc_comp_bk/'\n",
    "PATH_MAIN = '/home/ubuntu/NYC_Taxi/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(PATH_MAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Work_On_Bucket():\n",
    "    \n",
    "    \"\"\"\n",
    "    This class makes possible the connectivity between Jupyter and GClood\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name):\n",
    "        \n",
    "        \"\"\"\n",
    "        Here is where the conenction is established, a new variable is created for the connected bucket\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        bucket_name : str\n",
    "            The name of the Bucket with whom you want to communicate\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # Get access to the bucket:\n",
    "        storage_client = storage.Client()\n",
    "        self.bucket = storage_client.get_bucket(bucket_name)\n",
    "        \n",
    "    def get_file_from_bucket(self, file_name, save_path, bucket_folder=''):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        This function retrieves a file/folder from the connected bucket and save it locally on Jupyter\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        file_name : str\n",
    "            The name of the file to get from the bucket\n",
    "        save_path : str\n",
    "            The path where to save the file on Jupyter\n",
    "            (ex: /ubuntu/user/my_folder/)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # Download the file:\n",
    "        blob = self.bucket.blob(bucket_folder+file_name)\n",
    "        blob.download_to_filename(''.join([save_path, file_name]))\n",
    "            \n",
    "    def upload_file_to_bucket(self, file_name, folder_path, bucket_folder=''):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function save a local Jupyter file to the connected Bucket\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        file_name : str\n",
    "            The name of the file tosave\n",
    "        folder_path : str\n",
    "            The path where to find the file on the Bucket\n",
    "        bucket_folder: str\n",
    "            the folder where to save file in the Bucket\n",
    "            \n",
    "        \"\"\"\n",
    "        # Upload the File\n",
    "        object_to_save = self.bucket.blob(bucket_folder+file_name)\n",
    "        object_to_save.upload_from_filename(folder_path + file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bucket = Work_On_Bucket('nyc_comp_bk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
     ]
    }
   ],
   "source": [
    "# Set kaggle:\n",
    "! mkdir ~/.kaggle\n",
    "Bucket.get_file_from_bucket('kaggle.json', PATH_MAIN)\n",
    "! cp /home/ubuntu/NYC_Taxi/kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading new-york-city-taxi-fare-prediction.zip to /home/ubuntu/NYC_Taxi\n",
      "100%|██████████████████████████████████████▉| 1.56G/1.56G [00:14<00:00, 128MB/s]\n",
      "100%|███████████████████████████████████████| 1.56G/1.56G [00:14<00:00, 119MB/s]\n",
      "Archive:  new-york-city-taxi-fare-prediction.zip\n",
      "  inflating: /home/ubuntu/NYC_Taxi/data/GCP-Coupons-Instructions.rtf  \n",
      "  inflating: /home/ubuntu/NYC_Taxi/data/sample_submission.csv  \n",
      "  inflating: /home/ubuntu/NYC_Taxi/data/test.csv  \n",
      "  inflating: /home/ubuntu/NYC_Taxi/data/train.csv  \n",
      "Start Uploding!\n",
      "Succesfully Uploaded!\n"
     ]
    }
   ],
   "source": [
    "# Download The Dataset\n",
    "!kaggle competitions download -c new-york-city-taxi-fare-prediction\n",
    "\n",
    "# Unzip the Files\n",
    "! unzip new-york-city-taxi-fare-prediction.zip -d /home/ubuntu/NYC_Taxi/data/\n",
    "! rm new-york-city-taxi-fare-prediction.zip\n",
    "\n",
    "# Upload databses to bucket:\n",
    "print('Start Uploding!')\n",
    "Bucket.upload_file_to_bucket('train.csv', PATH_MAIN + 'data/')\n",
    "Bucket.upload_file_to_bucket('test.csv', PATH_MAIN + 'data/')\n",
    "print('Succesfully Uploaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Steps (Load + Checks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data:\n",
    "data = spark.read.load(PATH_BUCKET+\"train.csv\", format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "test = spark.read.load(PATH_BUCKET+\"test.csv\", format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# Load Test (Because pyspark changes the timestamp):\n",
    "Bucket.get_file_from_bucket('test.csv', '')\n",
    "original_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is 55423856 rows by 8 columns\n"
     ]
    }
   ],
   "source": [
    "# Get DB shape:\n",
    "ncol = len(data.columns)\n",
    "nrow = data.count()\n",
    "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(nrow, ncol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: timestamp (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the schema:\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Key, not needed:\n",
    "data = data.drop('key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>55423856</td>\n",
       "      <td>55423856</td>\n",
       "      <td>55423856</td>\n",
       "      <td>55423856</td>\n",
       "      <td>55423480</td>\n",
       "      <td>55423480</td>\n",
       "      <td>55423856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>11.345045601663855</td>\n",
       "      <td>None</td>\n",
       "      <td>-72.50968444358728</td>\n",
       "      <td>39.91979178688818</td>\n",
       "      <td>-72.5112097297181</td>\n",
       "      <td>39.92068144482884</td>\n",
       "      <td>1.6853799201556816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>20.7108321982325</td>\n",
       "      <td>None</td>\n",
       "      <td>12.848883381402654</td>\n",
       "      <td>9.642353041994934</td>\n",
       "      <td>12.782196517830771</td>\n",
       "      <td>9.633345796415126</td>\n",
       "      <td>1.327664357095968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>2009-01-01 00:00:27 UTC</td>\n",
       "      <td>-3442.059565</td>\n",
       "      <td>-3492.263768</td>\n",
       "      <td>-3442.024565</td>\n",
       "      <td>-3547.886698</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>93963.36</td>\n",
       "      <td>2015-06-30 23:59:54 UTC</td>\n",
       "      <td>3457.625683</td>\n",
       "      <td>3408.789565</td>\n",
       "      <td>3457.62235</td>\n",
       "      <td>3537.132528</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary         fare_amount          pickup_datetime    pickup_longitude  \\\n",
       "0   count            55423856                 55423856            55423856   \n",
       "1    mean  11.345045601663855                     None  -72.50968444358728   \n",
       "2  stddev    20.7108321982325                     None  12.848883381402654   \n",
       "3     min              -300.0  2009-01-01 00:00:27 UTC        -3442.059565   \n",
       "4     max            93963.36  2015-06-30 23:59:54 UTC         3457.625683   \n",
       "\n",
       "     pickup_latitude   dropoff_longitude   dropoff_latitude  \\\n",
       "0           55423856            55423480           55423480   \n",
       "1  39.91979178688818   -72.5112097297181  39.92068144482884   \n",
       "2  9.642353041994934  12.782196517830771  9.633345796415126   \n",
       "3       -3492.263768        -3442.024565       -3547.886698   \n",
       "4        3408.789565          3457.62235        3537.132528   \n",
       "\n",
       "      passenger_count  \n",
       "0            55423856  \n",
       "1  1.6853799201556816  \n",
       "2   1.327664357095968  \n",
       "3                   0  \n",
       "4                 208  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show some basic Statistics:\n",
    "stats = data.select(data.columns).describe()\n",
    "stats.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_datetime: 0\n",
      "pickup_longitude: 0\n",
      "pickup_latitude: 0\n",
      "dropoff_longitude: 376\n",
      "dropoff_latitude: 376\n",
      "passenger_count: 0\n"
     ]
    }
   ],
   "source": [
    "# Check Nulls:\n",
    "for c in data.columns[1:]:\n",
    "    nans = data.where(col(c).isNull()).count()\n",
    "    print('{:s}: {:d}'.format(c, nans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Rows with Missing Values:\n",
    "data = data.na.drop(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duplicates are: 1650\n"
     ]
    }
   ],
   "source": [
    "# Check Duplicates:\n",
    "print('The Duplicates are: {:d}'.format(data.count()-data.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates:\n",
    "data = data.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the EDA check the dashboard out on the git repository: https://github.com/CalenDario13/NYC_Taxi/blob/main/nyc_comp.twb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Latitude and Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the study of the quantiles it is clear that there are many outlayers, because the NYC coordinates are (lat=40.730610 lon=-73.935242). Even working with quantiles, removing anything out of the I and III quartile things don't change so much. \n",
    "Therefore, I opt for anothe option: I draw a triangle around the area of interest and anything outside the triangle will be dropped (because NY City is on an icleand which reminds a triangle). To take into acocunt possible trips from/to the countriside I am using larger bounderies. To decide whether a point is in the selected area I walk clockwise/counterclockwise around the triangle and project the point onto the segment I am crossing by using the dot product. Finally, I check that the vector created is on the same side for each of the triangle's segments.\n",
    "To have a more permissive approach, it is possible to draw a rectangle and include more points: It seems that there are many taxis that go and come from/to the states placed at the North of NYC. \n",
    "\n",
    "There is another issues linked with the coordinates, there are some points (both for pick up and drop off) which are located in the ocean, There are different libraries that can help in removing them, by masking the map, but it takes too much computational resources. Another possibility is to define a more complex shape (instead of a triangle) that defines bounderies more accurately, but this option is too expensive in terms of computation, too (no easy way to compute if a point is in a complex geometric figure). Not having any alternatives I keep them hopping that they won't affect my model too much. \n",
    "\n",
    "For what concerns how pick up and drop off locations are distributed in the map, as I expected they are mostly concentrated in Manhattan. Howevere, there are some areas outside it, which seems to have an important tarffic of Taxi: JFK Airport, East Elmhurst, the Newy York Liberty Airport and a point in Hamilton Town in New Jersy. \n",
    "It is possible to check that drop off locations are more spread all along NYC and countyside than pickup ones that are more concentrated in Manhattan. Moreover, it seems that the price changes more depending on the dropoff position than the pickup one: the avarage fare amount for a trip seems to increase the further the trip ends outised Mantthan and if a trip starts or stops from/in any NY Airport, checking on the web the routes for the airports have a fixed price (https://www.jfkairport.com/to-from-airport/taxi-car-and-van-service). It is interesting to see how the Fare Amount doesn't depend on the distance from the pickup location for the races towards the airports, instead for the journeys from the airport there is a linear relationship between the distance and the fare amount. For this reason I compute a categorical variable ad hoc, which compute the haversine distance from the airports for both the pickup and dropoff position and if one of them is enough small (enough close) to an airport it shows this with the corresponding label, otherwise it assume that it is a normal trip.\n",
    "\n",
    "Finally, it seems that some trips are repeted more frequently, the mode is 1, but for some observations the count can arrive up to 36 (and potentially more, for the full dataset). This suggest me that there are some people tha use it regularly, they may get a discount for that reason, a kind of subscription. Given that they are very few, for the moment I am not going to create a variable ad hoc (taking also into account that the data are spread over 6 years, 36 times is not such a significant amount).\n",
    "\n",
    "At this point, it is possible to compute new features that can be useful both for the model and to remove more outlayers. All the distances are expressed in kilometers. To obtain some of them I need to know how much Kms correspond to the variation of 1 degree for each coordinate and it turns out that for Latitude it is 111 Km and for Longitude is 85 Km; and the radius of the hearth in kilometers (R=6371Km):\n",
    " \n",
    " - Absolute latitude/longitude Distance:\n",
    "    <br>\n",
    "    <br>\n",
    "    $abs.lat = | Lat_2 - Lat_1 | * 111$\n",
    "    <br>\n",
    "    $abs.long = | Long_2 - Long_1 | * 85$\n",
    "    <br>\n",
    "    <br>\n",
    " - Haversinee Distance:\n",
    "    <br>\n",
    "    <br>\n",
    "    $a = sin^2(\\frac{\\Delta lat}{2}) + cos(lat_1)*cos(lat_2)*sin^2(\\frac{\\Delta long}{2})$\n",
    "    <br>\n",
    "    $c = 2 * atan2(\\sqrt{a},\\sqrt{1-a})$\n",
    "    <br>\n",
    "    $haverine = R * c$\n",
    "    <br>\n",
    "    <br>\n",
    " - Manhattan Distance:\n",
    "    <br>\n",
    "    <br>\n",
    "    $manhattan = abs.lat + abs.long$\n",
    "    <br>\n",
    "    <br>\n",
    " - Euclidean distance:\n",
    "    <br>\n",
    "    <br>\n",
    "    $euclidean = \\sqrt{abs.lat^2 + abs.long^2}$\n",
    "    <br>\n",
    "    <br>\n",
    " - Direction:\n",
    "    <br>\n",
    "    <br>\n",
    "    $y = sin(\\Delta long) * cos(lat_2)$\n",
    "    <br>\n",
    "    $x = cos(lat_1) * sin(lat_2) - sin(lat_1) * cos(lat_2) * cos(\\Delta lon)$\n",
    "    <br>\n",
    "    $direction = atan2(y, x)*\\frac{180}{\\pi}$\n",
    "    <br>\n",
    "    \n",
    "For what concerns the distance metrics, there are a lot of zeros (to which sometimes correspond also high fare amount), these values are probably missing coordinates replaced with zeros or figures wrongly reported. To overcome this issue I am going to drop any row which has the mean between the three compute distances (Euclidean, Manhattan and Haversine) less than 0.500 (namely less than 500 meter). \n",
    "The distributions of all the distances are very close each other, they have a mode around 1 Km and they are positive skwed, it is possible to see that the most of the races last aroun 0.5 and 7.0 Kms. \n",
    "It is also possible to see a positive relationship between the fare amount and the average kilometers of the journey, the increase in the average of kilometers is more than proportional to the fare amount. To simplify this idea, I create a categorical variable which says whethe a trip is longer or shorter or equal than the average and it seems that my hypothesis is confirmed: the average fare amount for longer trips is far away higher than the one for the shorter trips.\n",
    "I can then compute the dropoff and pickup distance from Manhatta (lat=40.754932, long=-73.984016), which seems to be the point where the most of taxis start and end their races, using the haverisne distance.\n",
    "Another feature that can improve my model is the direction (in degree) of the trip. Because the computed distancem is only an approximation and the angle can improve this approximation. To comput the distance I use the Bearing Algorithm and I multiply the result by -1 because of the negative longitude. It doesn't seems to have a direct impact on the average fare amount, but maybe (expecially in a linear model) I can multiply this variable with the distances to reduce the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3442.059565,  2123.82246 ,  3457.625683],\n",
       "       [-3492.263768,   444.133332,  3408.789565],\n",
       "       [-3442.024565,   -73.980088,  3457.62235 ],\n",
       "       [-3547.886698,  2599.28739 ,  3537.132528]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check I, II and III Quantiles:\n",
    "np.array([data.approxQuantile('pickup_longitude', [0.25, 0.50, 0.75], 0.25),\n",
    "          data.approxQuantile('pickup_latitude', [0.25, 0.50, 0.75], 0.25),\n",
    "          data.approxQuantile('dropoff_longitude', [0.25, 0.50, 0.75], 0.25),\n",
    "          data.approxQuantile('dropoff_latitude', [0.25, 0.50, 0.75], 0.25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write function to check if a point is a triangle:\n",
    "\n",
    "def point_in_triangle(lat, long):\n",
    "    \n",
    "    \"\"\"\n",
    "    phuclv (https://stackoverflow.com/questions/2049582/how-to-determine-if-a-point-is-in-a-2d-triangle)\n",
    "    To decide whether a point is in the triangle walk clockwise or counterclockwise around the triangle and \n",
    "    project the point onto the segment crossing by using the dot product. \n",
    "    Finally, check that the vector created is on the same side for each of the triangle's segments.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lat : float\n",
    "        The latitude coordinate of a point\n",
    "    long :float\n",
    "        The longitude oordinate of a point\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        whether all signs of the sides are positive/negative\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack arguments\n",
    "    x = lat\n",
    "    y = long\n",
    "    ax, ay = (39.74274655286439, -75.26399018397262)\n",
    "    bx, by = (41.58840050439113, -75.18708588709761)\n",
    "    cx, cy = (41.07279488244855, -71.76484467616011)\n",
    "    # Segment A to B\n",
    "    side_1 = (x - bx) * (ay - by) - (ax - bx) * (y - by)\n",
    "    # Segment B to C\n",
    "    side_2 = (x - cx) * (by - cy) - (bx - cx) * (y - cy)\n",
    "    # Segment C to A\n",
    "    side_3 = (x - ax) * (cy - ay) - (cx - ax) * (y - ay)\n",
    "    # All the signs must be positive or all negative\n",
    "    return (side_1 < 0.0) == (side_2 < 0.0) == (side_3 < 0.0)\n",
    "\n",
    "def is_in_area(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Check whether both pick up and drop off points of an observation\n",
    "    are in the triangle.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lat1 : float\n",
    "        The latitude coordinate of a pick up point\n",
    "    long1 :float\n",
    "        The longitude oordinate of a pick up point\n",
    "        \n",
    "    lat2 : float\n",
    "        The latitude coordinate of a drop off point\n",
    "    long2 :float\n",
    "        The longitude oordinate of a drop off point\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        whether both the points are in the hotspot\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    pick = point_in_triangle(lat1, lon1)\n",
    "    drop = point_in_triangle(lat2, lon2)\n",
    "    \n",
    "    return all([pick, drop])\n",
    "\n",
    "is_in_area_udf = F.udf(is_in_area, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any point outside the given area:\n",
    "data = data.withColumn('keep', is_in_area_udf( col('pickup_latitude'), col('pickup_longitude'), col('dropoff_latitude'), col('dropoff_longitude') ))\n",
    "data = data.filter(\"keep == True\")\n",
    "data = data.drop('keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+-------------------+--------------------+\n",
      "|summary|     pickup_latitude|   pickup_longitude|   dropoff_latitude|   dropoff_longitude|\n",
      "+-------+--------------------+-------------------+-------------------+--------------------+\n",
      "|  count|            54235168|           54235168|           54235168|            54235168|\n",
      "|   mean|   40.75084046363401| -73.97546359704323|  40.75123945667267|  -73.97456012998938|\n",
      "| stddev|0.027549044484391723|0.03574727580738844|0.03143901853685563|0.035618188092662054|\n",
      "|    min|           39.804188| -75.25401306152344|           39.80016|  -75.25403594970703|\n",
      "|    max|           41.519495|           -72.0619|          41.567147|           -71.97642|\n",
      "+-------+--------------------+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(col('pickup_latitude'), col('pickup_longitude'), col('dropoff_latitude'), col('dropoff_longitude')).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Functions needed to Create Coordinates Features\n",
    "\n",
    "class Coordinates_Transform():\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    This class allows to create new features exploting the latitude and longitude of the pickup and dropoff position\n",
    "    \n",
    "    Parameters\n",
    "        ----------\n",
    "        df : Pyspark Database\n",
    "            The database to use to make the transformations\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.df = df\n",
    "        self.haversine_udf = F.udf(Coordinates_Transform.haversine_stat, DoubleType())\n",
    "        self.direction_udf = F.udf(Coordinates_Transform.calculate_dir, DoubleType())\n",
    "        self.airports_udf = F.udf(Coordinates_Transform.identify_airports, StringType())\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def haversine_stat(pick_lat, drop_lat, pick_long, drop_long):\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        Compute the Haversine Distance\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pick_lat : float\n",
    "            The latitude coordinate of a pick up point\n",
    "        pick_long :float\n",
    "            The longitude oordinate of a pick up point\n",
    "\n",
    "        drop_lat : float\n",
    "            The latitude coordinate of a drop off point\n",
    "        drop_long :float\n",
    "            The longitude oordinate of a drop off point\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The Haversine Distance between the pick up point and the drop off one\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Transform in radians the coordinate:\n",
    "        longit_a, latit_a, longit_b, latit_b = map(math.radians, [pick_long,  pick_lat, drop_long, drop_lat])\n",
    "        \n",
    "        # Compute the delta between both the coordinates:\n",
    "        dist_longit = longit_b - longit_a\n",
    "        dist_latit = latit_b - latit_a\n",
    "        \n",
    "        # Calculate area\n",
    "        area = math.sin(dist_latit/2)**2 + math.cos(latit_a) * math.cos(latit_b) * math.sin(dist_longit/2)**2\n",
    "        # Calculate the central angle\n",
    "        central_angle = 2 * math.asin(math.sqrt(area))\n",
    "\n",
    "        # Calculate Distance\n",
    "        RADIUS = 6371\n",
    "        distance = central_angle * RADIUS\n",
    "        return round(distance, 3)\n",
    "\n",
    "    def add_dist_metrics(self):\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        Compute Some distance metrics, such as: \n",
    "            - Absolute Distance \n",
    "            - Manahattan Distance\n",
    "            - Euclidean Distance \n",
    "            - Haversine Distance.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Spark Database\n",
    "            A Spark Databse containing the new features\n",
    "\n",
    "        \"\"\"\n",
    "       \n",
    "        # Absolute Lat e Long Distance:\n",
    "        self.df = self.df.withColumn( 'abs_dist_longitude', F.round( F.abs( col('pickup_longitude') - col('dropoff_longitude') ) * 85, 3 ) )\n",
    "        self.df = self.df.withColumn( 'abs_dist_latitude', F.round( F.abs( col('pickup_latitude') - col('dropoff_latitude') ) * 111 , 3 ) )\n",
    "\n",
    "        # Manhattan Distance:\n",
    "        self.df = self.df.withColumn( 'manhattan_dist', F.round( col('abs_dist_longitude') + col('abs_dist_latitude'), 3 ) )\n",
    "\n",
    "        # Euclidean Distance:\n",
    "        self.df = self.df.withColumn( 'euclidean_dist',  F.round( F.sqrt( col('abs_dist_longitude')**2 + col('abs_dist_latitude')**2 ), 3 ) )\n",
    "\n",
    "        # Haversine Distance:\n",
    "        self.df = self.df.withColumn( 'haversine_dist', \n",
    "                                     self.haversine_udf( col('pickup_latitude'), col('dropoff_latitude'), col('pickup_longitude'), col('dropoff_longitude') ) ) \n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    def distance_from_center(self):\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        It computes the haversine distance for both pickup and dropoff position from the city center\n",
    "        which I choose to be Manhattan.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Spark Databse\n",
    "            A database containing all the older variables plus the new one\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        MANHATTAN_LAT = F.lit(40.754932)\n",
    "        MANHATTAN_LON = F.lit(-73.984016)\n",
    "        \n",
    "        self.df = self.df.withColumn( 'pickup_from_center', \n",
    "                                     self.haversine_udf( MANHATTAN_LAT, col('pickup_latitude'), MANHATTAN_LON, col('pickup_longitude') ) )\n",
    "        self.df = self.df.withColumn( 'dropoff_from_center', \n",
    "                                     self.haversine_udf( MANHATTAN_LAT, col('dropoff_latitude'), MANHATTAN_LON, col('dropoff_longitude') ) )\n",
    "\n",
    "        return self.df\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_dir(lat1, lon1, lat2, lon2):\n",
    "\n",
    "        '''\n",
    "        \n",
    "        https://www.movable-type.co.uk/scripts/latlong.html\n",
    "        It is measured in 0 - 360 degrees\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lat1 : float\n",
    "            The latitude coordinate of a pick up point\n",
    "        lon1 :float\n",
    "            The longitude oordinate of a pick up point\n",
    "\n",
    "        lat : float\n",
    "            The latitude coordinate of a drop off point\n",
    "        lon2 :float\n",
    "            The longitude oordinate of a drop off point\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The computed degree between 0 and 360\n",
    "        \n",
    "        '''\n",
    "\n",
    "        dlon = lon2 - lon1\n",
    "        y = math.sin(dlon) * math.cos(lat2)\n",
    "        x = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(dlon)\n",
    "        theta = math.atan2(y, x)*180/math.pi\n",
    "\n",
    "        return (-theta + 360) % 360\n",
    "\n",
    "\n",
    "\n",
    "    def direction(self):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        It executes the function to compute the degree on the database of the clas\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Spark Databse\n",
    "            A database containing all the older variables plus the new one\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.df = self.df.withColumn('direction', self.direction_udf( col('pickup_latitude'), col('pickup_longitude'), col('dropoff_latitude'), col('dropoff_longitude') ))\n",
    "        return self.df\n",
    "\n",
    "    def third_dim_remap(self):\n",
    "        \n",
    "        '''\n",
    "        The credit for the next features goes to Jan van der Vegt @datascience.stackexchange.com\n",
    "        https://datascience.stackexchange.com/users/14904/jan-van-der-vegt\n",
    "        \n",
    "        It recomputes the coordinates in 3D. It is useful to be more precise and when I am going to work with trees based models.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Spark Databse\n",
    "            A database containing all the older variables plus the new one\n",
    "            \n",
    "        '''\n",
    "        # Compute coordinates for pickup:\n",
    "        self.df = self.df.withColumn('pickup_x', F.cos('pickup_latitude') * F.cos('pickup_longitude'))\n",
    "        self.df = self.df.withColumn('pickup_y', F.cos('pickup_latitude') * F.sin('pickup_longitude'))\n",
    "        self.df = self.df.withColumn('pickup_z', F.sin('pickup_latitude'))\n",
    "\n",
    "        # Compute coordinates for dropoff\n",
    "        self.df = self.df.withColumn('dropoff_x', F.cos('dropoff_latitude') * F.cos('dropoff_longitude'))\n",
    "        self.df = self.df.withColumn('dropoff_y', F.cos('dropoff_latitude') * F.sin('dropoff_longitude'))\n",
    "        self.df = self.df.withColumn('dropoff_z', F.sin('dropoff_latitude'))\n",
    "\n",
    "        return self.df\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def identify_airports(pick_lat, drop_lat, pick_long, drop_long):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        This function identifies whether a journey starts or ends near by an airport,\n",
    "        It computes the haversine distance both for the pickup position and dropoof position\n",
    "        and the airports, if the distance is lower than 2 Kms it assigns the corresponding label.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        pick_lat : float\n",
    "            The latitude coordinate of a pick up point\n",
    "        pick_long :float\n",
    "            The longitude oordinate of a pick up point\n",
    "\n",
    "        drop_lat : float\n",
    "            The latitude coordinate of a drop off point\n",
    "        drop_long :float\n",
    "            The longitude oordinate of a drop off point\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            label to identify if a trip is started/ended at the airports\n",
    "        \n",
    "        '''\n",
    "\n",
    "        # Set the coordinates of airports:\n",
    "        JFK_LAT = 40.641766\n",
    "        JFK_LON = -73.780968\n",
    "\n",
    "        LGR_LAT = 40.773013\n",
    "        LGR_LON = -73.870229\n",
    "\n",
    "        EWR_LAT = 40.689531\n",
    "        EWR_LON = -74.174462\n",
    "\n",
    "        # Compute distances:\n",
    "        pick_up_jfk = Coordinates_Transform.haversine_stat(JFK_LAT, pick_lat, JFK_LON, pick_long)\n",
    "        drop_off_jfk = Coordinates_Transform.haversine_stat(drop_lat, JFK_LAT, drop_long, JFK_LON)\n",
    "\n",
    "        pick_up_lgr = Coordinates_Transform.haversine_stat(LGR_LAT, pick_lat, LGR_LON, pick_long)\n",
    "        drop_off_lgr = Coordinates_Transform.haversine_stat(drop_lat, LGR_LAT, drop_long, LGR_LON)\n",
    "\n",
    "        pick_up_ewr = Coordinates_Transform.haversine_stat(EWR_LAT, pick_lat, EWR_LON, pick_long)\n",
    "        drop_off_ewr = Coordinates_Transform.haversine_stat(drop_lat, EWR_LAT, drop_long, EWR_LON)\n",
    "\n",
    "        print(pick_up_jfk, drop_off_jfk)\n",
    "        # Assign a value:\n",
    "        if pick_up_jfk < 2:\n",
    "            return 'PICK_JFK'\n",
    "        elif drop_off_jfk < 2:\n",
    "            return 'DROP_JFK'\n",
    "        if pick_up_lgr < 2:\n",
    "            return 'PICK_LGR'\n",
    "        elif drop_off_lgr < 2:\n",
    "            return 'DROP_LGR'\n",
    "        if pick_up_ewr < 2:\n",
    "            return 'PICK_EWR'\n",
    "        elif drop_off_ewr < 2:\n",
    "            return 'DROP_EWR'\n",
    "        else:\n",
    "            return 'NO_AIRPORT'\n",
    "\n",
    "    def airports(self):\n",
    "        \n",
    "        '''\n",
    "        It executes the function to identify trips started/ended near by the airports\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Spark Databse\n",
    "            A database containing all the older variables plus the new one\n",
    "            \n",
    "        '''\n",
    "\n",
    "        self.df = self.df.withColumn('airport', \n",
    "                                     self.airports_udf( col('pickup_latitude'), col('dropoff_latitude'), col('pickup_longitude'), col('dropoff_longitude') ))\n",
    "        return self.df\n",
    "\n",
    "    def long_short_trip(self):\n",
    "        \n",
    "        '''\n",
    "        It identifies whether a trip is longer or shorter than the avergare haversine distance.\n",
    "        Possible improvments: use the mode or a value picked ad hoc from the distribution such as 7\n",
    "        (the second improvment is also in terms of speed of computation).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Spark Databse\n",
    "            A database containing all the older variables plus the new one\n",
    "            \n",
    "        '''\n",
    "\n",
    "        mean_len = self.df.select('haversine_dist').agg(F.mean(col('haversine_dist'))).first()[0]\n",
    "        self.df = self.df.withColumn( 'long_short', F.when(col('haversine_dist') <= mean_len, 'short') \\\n",
    "                                     .otherwise('long') )\n",
    "        return self.df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Distance metrics:\n",
    "coordTransform = Coordinates_Transform(data)\n",
    "data = coordTransform.add_dist_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>abs_dist_longitude</th>\n",
       "      <th>abs_dist_latitude</th>\n",
       "      <th>manhattan_dist</th>\n",
       "      <th>euclidean_dist</th>\n",
       "      <th>haversine_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>54235168</td>\n",
       "      <td>54235168</td>\n",
       "      <td>54235168</td>\n",
       "      <td>54235168</td>\n",
       "      <td>54235168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>1.9286066518499425</td>\n",
       "      <td>2.357046155476089</td>\n",
       "      <td>4.28565280732606</td>\n",
       "      <td>3.3246101278786595</td>\n",
       "      <td>3.315646530605393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>2.887787858818069</td>\n",
       "      <td>2.5609968462258927</td>\n",
       "      <td>4.783709562936858</td>\n",
       "      <td>3.6221773285676875</td>\n",
       "      <td>3.607593005204814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>158.809</td>\n",
       "      <td>128.346</td>\n",
       "      <td>210.365</td>\n",
       "      <td>163.92</td>\n",
       "      <td>162.283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary  abs_dist_longitude   abs_dist_latitude     manhattan_dist  \\\n",
       "0   count            54235168            54235168           54235168   \n",
       "1    mean  1.9286066518499425   2.357046155476089   4.28565280732606   \n",
       "2  stddev   2.887787858818069  2.5609968462258927  4.783709562936858   \n",
       "3     min                 0.0                 0.0                0.0   \n",
       "4     max             158.809             128.346            210.365   \n",
       "\n",
       "       euclidean_dist     haversine_dist  \n",
       "0            54235168           54235168  \n",
       "1  3.3246101278786595  3.315646530605393  \n",
       "2  3.6221773285676875  3.607593005204814  \n",
       "3                 0.0                0.0  \n",
       "4              163.92            162.283  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's Check df:\n",
    "data.select('abs_dist_longitude', 'abs_dist_latitude', 'manhattan_dist', 'euclidean_dist', 'haversine_dist').describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove outlayers:\n",
    "DIST_METRICS = ['manhattan_dist', 'euclidean_dist', 'haversine_dist']\n",
    "n = len(DIST_METRICS)\n",
    "row_mean  = (sum(col(x) for x in DIST_METRICS) / n).alias(\"mean_dist\")\n",
    "data = data.where( (row_mean >= 0.5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>abs_dist_longitude</th>\n",
       "      <th>abs_dist_latitude</th>\n",
       "      <th>manhattan_dist</th>\n",
       "      <th>euclidean_dist</th>\n",
       "      <th>haversine_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>52277799</td>\n",
       "      <td>52277799</td>\n",
       "      <td>52277799</td>\n",
       "      <td>52277799</td>\n",
       "      <td>52277799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>1.9966080829072308</td>\n",
       "      <td>2.4408030829874807</td>\n",
       "      <td>4.437411165894727</td>\n",
       "      <td>3.4421882540617297</td>\n",
       "      <td>3.4329118362653372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>2.919377212362032</td>\n",
       "      <td>2.5708288215629684</td>\n",
       "      <td>4.806322907125751</td>\n",
       "      <td>3.636923794344817</td>\n",
       "      <td>3.6221358994083372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>158.809</td>\n",
       "      <td>128.346</td>\n",
       "      <td>210.365</td>\n",
       "      <td>163.92</td>\n",
       "      <td>162.283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary  abs_dist_longitude   abs_dist_latitude     manhattan_dist  \\\n",
       "0   count            52277799            52277799           52277799   \n",
       "1    mean  1.9966080829072308  2.4408030829874807  4.437411165894727   \n",
       "2  stddev   2.919377212362032  2.5708288215629684  4.806322907125751   \n",
       "3     min                 0.0                 0.0                0.5   \n",
       "4     max             158.809             128.346            210.365   \n",
       "\n",
       "       euclidean_dist      haversine_dist  \n",
       "0            52277799            52277799  \n",
       "1  3.4421882540617297  3.4329118362653372  \n",
       "2   3.636923794344817  3.6221358994083372  \n",
       "3                0.44               0.438  \n",
       "4              163.92             162.283  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select('abs_dist_longitude', 'abs_dist_latitude', 'manhattan_dist', 'euclidean_dist', 'haversine_dist').describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Directions:\n",
    "data = coordTransform.direction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add third dimension:\n",
    "data = coordTransform.third_dim_remap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Add when starting point or arrival is at the airport:\n",
    "data = coordTransform.airports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------------+------------------+\n",
      "|   airport|  avg(fare_amount)|stddev_samp(fare_amount)|count(fare_amount)|\n",
      "+----------+------------------+------------------------+------------------+\n",
      "|NO_AIRPORT| 9.805857909599204|      20.095290311293294|          51245784|\n",
      "|  PICK_LGR| 30.31977970052851|      11.499853431533305|           1099004|\n",
      "|  DROP_LGR|30.947311994375728|       7.715635313183146|            671323|\n",
      "|  PICK_JFK|43.932558035713896|      16.581543542956524|            810880|\n",
      "|  DROP_JFK| 50.35574337028614|       8.367285405592215|            337194|\n",
      "|  PICK_EWR|60.988619949248466|      36.609269575459976|              5123|\n",
      "|  DROP_EWR|  70.9053423929548|      14.149131512730918|             65860|\n",
      "+----------+------------------+------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check avg price for the airports:\n",
    "data.select('airport','fare_amount').groupBy('airport').agg(F.mean('fare_amount'), F.stddev('fare_amount'), F.count('fare_amount')).sort('avg(fare_amount)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Pickup and Dropoff distance from the city center:\n",
    "data = coordTransform.distance_from_center()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-----------+\n",
      "|pickup_from_center|dropoff_from_center|fare_amount|\n",
      "+------------------+-------------------+-----------+\n",
      "|             1.407|              2.707|       11.0|\n",
      "|              1.32|               1.35|        5.5|\n",
      "|             0.496|               2.34|        8.1|\n",
      "|             0.802|              4.085|       12.5|\n",
      "|             5.746|              2.029|       14.9|\n",
      "+------------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"pickup_from_center\", \"dropoff_from_center\", \"fare_amount\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a binning for the length, based on the haversine distance:\n",
    "data = coordTransform.long_short_trip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------------------------+------------------+\n",
      "|long_short| avg(fare_amount)|stddev_samp(fare_amount)|count(fare_amount)|\n",
      "+----------+-----------------+------------------------+------------------+\n",
      "|      long|19.82420825373721|      22.274972965933255|          16769255|\n",
      "|     short| 7.52029804345085|      18.959355627956782|          37465913|\n",
      "+----------+-----------------+------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check avg price for binned trip length:\n",
    "data.select('long_short' ,'fare_amount').groupBy('long_short').agg(F.mean('fare_amount'), F.stddev('fare_amount'), F.count('fare_amount')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Time Variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pickup_datetime column it is possible to extract some information about the time at which the race happened, such as:\n",
    "\n",
    "    - Year\n",
    "    - Quarter\n",
    "    - Month\n",
    "    - Week of year\n",
    "    - Day of Month\n",
    "    - Day of Week\n",
    "    - Day of Year\n",
    "    - Hour\n",
    "    - Minutes\n",
    "    - Seconds\n",
    "    \n",
    "Without going further, it is already possible to extract some useful information from this data. The average Fare amount for a race have been increased during the years, in particular from 2011 to 2013 it had grew by 2 dollars. \n",
    "The Fare Amount during the year has an increasing trend: it has its bottom in January, then increases till May - June, then it has drop in July-August and then it grows again in September and stays stable till the end of the year. This trend is confirmed by the anlysis of quarters, which highlights how the second one has the highest average fare amount and also the highest frequency. This trend can be justified by the increase of the number of travelling people in that period. In support of my thesis, if I keep only the trips from/to the airports, the max number of races are in the second quarter, this highlights that in that period more poeple travel. Anyway, also the frequency of the races inside the city grows in that perios, so I can suppose that more poeple are in the city during that period. \n",
    "Even if the first month of year seems to be the one with the lowest Fare Amount, the first week of the year is an exception, it has the third highest average fare amoutn, it is probable due to the New Year Eves.\n",
    "Finally, it is possible to check that the average fare amount is very high during the night, expecially at 5am. But it is not a real increase in the price, it happens because the trips are longer on average at the time, I can suppose that this races bring people home after parties or to work during the morining. In particular, seems that at that time there is a hsotpot in the JFK airport, many people are picked up from there. Instead the average fare per kilometer (namely an increase of the price for each kilometer done) happens later: from 8am to 4pm, the fare amount is the highest.\n",
    "\n",
    "Checking on the web (https://www1.nyc.gov/site/tlc/passengers/taxi-fare.page) i have found that at different part of the day, there price for a race can chaneg due to the fact to different fees to pay. For this purpose I have created a categorical variable ad hoc, which divides the day in 5 parts (morning, afternoon, rush hour, evening and overnight). The most of the races happen overnight and during the morining, but it seems that the average fare amount is stable during the whole day. \n",
    "It is also interesting to check whether the average fare amount changes during the weekend, also in this case the answer is negative, but I can say that the most of the races happen during the working days (Mon - Fri).\n",
    "\n",
    "Even if DateTime features are cyclical variables, when I extract them I loose this kind of information, I just have an ascending order of data points. To take into account this information I can make some transformatiosn to my data using two different periodic functions:\n",
    "\n",
    "<center>\n",
    "<br>\n",
    "    \n",
    "$xsin = sin(\\frac{2\\pi x}{max(x)})$\n",
    "    \n",
    "<br>\n",
    "    \n",
    "$xcos = cos(\\frac{2\\pi x}{max(x)})$\n",
    "    \n",
    "<br>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "    \n",
    "However, this approach doesn't work well for decision trees based algorithms (Random Forest, Gradient Boosted Trees, XGBoost) which build their split rules according to one feature at a time. An alternative way to transform date time features in continious way and in some way to catch the cycle, it is possible to opt for the fraction of the day/week/year/... . These variables have the advantage to work well with the non-linear models. These features can be easily computed as the sum of all the components of that datetime feature converted to the same unit, for example the fraction of the day is expressed in hours and is:\n",
    "\n",
    "<center>\n",
    "\n",
    "    \n",
    "$fracOfDay = hour + \\frac{minutes}{60} + \\frac{seconds}{3600}$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time_Transform():\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    This class extracts any interesting time features from the the pickup_datetime\n",
    "    \n",
    "    Parameters\n",
    "        ----------\n",
    "        df : Pyspark Database\n",
    "            The database to use to make the transformations\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Takes in input the databse to use and transformm the column pickup_datetime\n",
    "        in the DateTime format to extract what I need\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.df = df.withColumn('pickup_datetime', F.substring(col('pickup_datetime'), 1, 19))\\\n",
    "        .withColumn('pickup_datetime', F.to_timestamp(col('pickup_datetime')))\n",
    "        \n",
    "\n",
    "    def get_date(self, remove_original=False):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        It extracts all the basics information from the the datime,\n",
    "        (such as: year, month, day of the week, day of the year, ...)\n",
    "       \n",
    "       Parameters\n",
    "        ----------\n",
    "        \n",
    "        remove_original : bool\n",
    "            if True it removes the pickup_datetime column\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Spark Databse\n",
    "            the old databse with the new variables\n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.df = self.df.withColumn('year', F.year(col('pickup_datetime'))) \\\n",
    "        .withColumn('month', F.month(col('pickup_datetime'))) \\\n",
    "        .withColumn('week_of_year', F.weekofyear(col('pickup_datetime'))) \\\n",
    "        .withColumn('quarter', F.quarter(col('pickup_datetime')))\\\n",
    "        .withColumn('day_of_month', F.dayofmonth(col('pickup_datetime'))) \\\n",
    "        .withColumn('day_of_week', F.dayofweek(col('pickup_datetime'))) \\\n",
    "        .withColumn('day_of_year', F.dayofyear(col('pickup_datetime'))) \\\n",
    "        .withColumn('hour', F.hour(col('pickup_datetime'))) \\\n",
    "        .withColumn('minute', F.minute(col('pickup_datetime'))) \\\n",
    "        .withColumn('second', F.second(col('pickup_datetime')))\n",
    "\n",
    "        if remove_original:\n",
    "            self.df = self.df.drop('pickup_datetime')\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def is_weekend(self):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        It creates a new column which identifaies whether the trip was in the weekend or not\n",
    "        \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Spark Database\n",
    "            the old databse with the new variable\n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.df = self.df.withColumn('is_week_end', F.when( (col('day_of_week')==6) | (col('day_of_week')==7), 'True' )\\\n",
    "                                    .otherwise('False'))\n",
    "        return self.df\n",
    "\n",
    "\n",
    "    def divide_day(self):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        It creates a new column which identifaies in which part of the day the trip was done.\n",
    "        \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Spark Database\n",
    "            the old databse with the new variable\n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.df = self.df.withColumn('part_of_day', F.when(col('hour')>=20, 'overnight')\\\n",
    "                                     .when((col('hour')>=0) & (col('hour')<=6), 'overnight')\\\n",
    "                                     .when((col('hour')>6) & (col('hour')<=12), 'morning')\\\n",
    "                                     .when((col('hour')>12) & (col('hour')<16), 'afternoon')\\\n",
    "                                     .when((col('hour')>=16) & (col('hour')<20) & (col('is_week_end')=='False'), 'rush_hour')\\\n",
    "                                     .when((col('hour')>=16) & (col('hour')<20) & (col('is_week_end')=='True'), 'evening'))\n",
    "\n",
    "        return self.df\n",
    "\n",
    "\n",
    "    def add_fractions(self, remove_original=False, remove_cat=False):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        It transforms some of the DateTime vatiables from Discrete to Continious using the idea of fraction of the whole.\n",
    "        Ex. frac_of_day identifies in which fraction of day the race occurs: ( (race.hour + race.minute/60 + race.second/3600) / tot.hour )\n",
    "        \n",
    "        This approach works well in practice with non-linear models.\n",
    "       \n",
    "       Parameters\n",
    "        ----------\n",
    "        \n",
    "        remove_original : bool\n",
    "            if True it removes the pickup_datetime column\n",
    "        remove_cat : bool\n",
    "            if True it removes all the original variables extracted from pickup_datetime\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Spark Databse\n",
    "            the old databse with the new variables\n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.df = self.df.withColumn('frac_of_day', ( col('hour') + col('minute')/60 + col('second')/3600 )/24 )\\\n",
    "        .withColumn('frac_of_week',  (col('day_of_week') + col('frac_of_day') )/7) \\\n",
    "        .withColumn('frac_of_month', ( col('day_of_month') + col('frac_of_day') ) / ( F.dayofmonth(F.last_day( col('pickup_datetime') ))+1 ) )\\\n",
    "        .withColumn('frac_of_year', ( col('day_of_year') + col('frac_of_day') )/(365 + 1) )     \n",
    "\n",
    "        if remove_original:\n",
    "            self.df = self.df.drop('pickup_datetime')\n",
    "        if remove_cat:\n",
    "            self.df = self.df.drop('month', 'week_of_year', 'quarter', 'day_of_month', 'day_of_week', 'day_of_year', 'hour',\n",
    "                                   'minute', 'second')\n",
    "        return self.df\n",
    "\n",
    "    def cyclical_encoding(self, remove_original=False, remove_cat=False):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        https://towardsdatascience.com/cyclical-features-encoding-its-about-time-ce23581845ca\n",
    "        https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning\n",
    "        A common method for encoding cyclical data is to transform the data into two dimensions using a sine and consine transformation.\n",
    "        We can do that using the following transformations:\n",
    "\n",
    "        xsin=sin(2∗π∗x/max(x)) \n",
    "        xcos=cos(2∗π∗x/max(x))\n",
    "        \n",
    "        It is useful to transform discrete variables in continious ones. This encoding can be useful when work with linear model.\n",
    "        (DON'T USE FOR NON LINEAR MODEL SUCH AS RANDOM FOREST AND DECISION TREES!)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        remove_original : bool\n",
    "            if True it removes the pickup_datetime column\n",
    "        remove_cat : bool\n",
    "            if True it removes all the original variables extracted from pickup_datetime\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Spark Databse\n",
    "            the old databse with the new variables\n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.df = self.df.withColumn('month_sin', F.sin(col('month') * 2*math.pi / 12))\\\n",
    "        .withColumn('month_cos', F.cos(col('month') * 2*math.pi / 12))\\\n",
    "        .withColumn('week_of_year_sin', F.sin(col('week_of_year') * 2*math.pi / 52))\\\n",
    "        .withColumn('week_of_year_cos', F.cos(col('week_of_year') * 2*math.pi / 52))\\\n",
    "        .withColumn('quarter_sin', F.sin(col('quarter') * 2*math.pi / 4))\\\n",
    "        .withColumn('quarter_cos', F.cos(col('quarter') * 2*math.pi / 4))\\\n",
    "        .withColumn('day_of_month_sin', F.sin(col('day_of_month') * 2*math.pi / 31))\\\n",
    "        .withColumn('day_of_month_cos', F.cos(col('day_of_month') * 2*math.pi / 31))\\\n",
    "        .withColumn('day_of_week_sin', F.sin(col('day_of_week') * 2*math.pi / 7))\\\n",
    "        .withColumn('day_of_week_cos', F.cos(col('day_of_week') * 2*math.pi / 7))\\\n",
    "        .withColumn('day_of_yearh_sin', F.sin(col('day_of_year') * 2*math.pi / 366))\\\n",
    "        .withColumn('day_of_year_cos', F.cos(col('day_of_year' ) * 2*math.pi / 366))\\\n",
    "        .withColumn('hour_sin', F.sin(col('hour') * 2*math.pi / 24))\\\n",
    "        .withColumn('hour_cos', F.cos(col('hour') * 2*math.pi / 24))\\\n",
    "        .withColumn('minute_sin', F.sin(col('minute') * 2*math.pi / 60))\\\n",
    "        .withColumn('minute_cos', F.cos(col('minute') * 2*math.pi / 60))\n",
    "\n",
    "        if remove_original:\n",
    "            self.df = self.df.drop('pickup_datetime')\n",
    "        if remove_cat:\n",
    "            self.df = self.df.drop('month', 'week_of_year', 'quarter', 'day_of_month', 'day_of_week', 'day_of_year', 'hour',\n",
    "                                   'minute', 'second')\n",
    "\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get daates basic info:\n",
    "timeTransform = Time_Transform(data)\n",
    "data = timeTransform.get_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----+------------+-------+------------+-----------+-----------+----+------+------+\n",
      "|    pickup_datetime|year|month|week_of_year|quarter|day_of_month|day_of_week|day_of_year|hour|minute|second|\n",
      "+-------------------+----+-----+------------+-------+------------+-----------+-----------+----+------+------+\n",
      "|2013-02-25 12:53:52|2013|    2|           9|      1|          25|          2|         56|  12|    53|    52|\n",
      "|2013-03-10 12:22:39|2013|    3|          10|      1|          10|          1|         69|  12|    22|    39|\n",
      "|2012-05-04 13:33:00|2012|    5|          18|      2|           4|          6|        125|  13|    33|     0|\n",
      "|2010-08-12 14:27:00|2010|    8|          32|      3|          12|          5|        224|  14|    27|     0|\n",
      "|2010-05-21 18:57:00|2010|    5|          20|      2|          21|          6|        141|  18|    57|     0|\n",
      "+-------------------+----+-----+------------+-------+------------+-----------+-----------+----+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('pickup_datetime','year','month', 'week_of_year', 'quarter','day_of_month', 'day_of_week','day_of_year','hour','minute','second').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it is weekend:\n",
    "data = timeTransform.is_weekend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|is_week_end|  avg(fare_amount)|\n",
      "+-----------+------------------+\n",
      "|      False|11.385143825938746|\n",
      "|       True|11.187080945739638|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('is_week_end', 'fare_amount').groupby('is_week_end').agg(F.mean('fare_amount')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check part of the day:\n",
    "data = timeTransform.divide_day()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|part_of_day|  avg(fare_amount)|\n",
      "+-----------+------------------+\n",
      "|  afternoon|11.768674577986591|\n",
      "|  overnight|11.508201533693835|\n",
      "|  rush_hour|11.170062041233122|\n",
      "|    morning|10.992560077359562|\n",
      "|    evening|10.994244561142468|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('part_of_day', 'fare_amount').groupby('part_of_day').agg(F.mean('fare_amount')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Time to be used better in the models:\n",
    "data = timeTransform.add_fractions()\n",
    "data = timeTransform.cyclical_encoding(remove_cat=False, remove_original=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Passenger count:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the NYC Taxi web site reports:* \"The maximum amount of passengers allowed in a yellow taxicab by law is four in a four passenger taxicab or five passengers in a five passenger taxicab, except that an additional passenger must be accepted if such passenger is under the age of seven and is held on the lap of an adult passenger seated in the rear. So I remove everything below zero and higher than 5. But I must keep into acocunt also the Van service, which from/to the airport can bring up to 15 pople, even if it is an extreme case (I may consider it as an outlayer let's see later on), for the moment I am going to keep up to 15 seats. It seems good to have kept also higher number of passenger, as matter of fact for a number of seats higher than 6 the average fare amount increases. I can also observe that there is lack for number of seats between 10 and 15. It seems that for a normal trip in a yellow taxi the fare amount is almost the same, instead for vans it positively increases with the number of passengers.\n",
    "To take into account this difference between Taxi and van I am goign to create an ad hoc variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|   passenger_count|\n",
      "+-------+------------------+\n",
      "|  count|          54235168|\n",
      "|   mean| 1.685401693602203|\n",
      "| stddev|1.3154630695998806|\n",
      "|    min|                 0|\n",
      "|    max|               208|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check my data:\n",
    "data.select('passenger_count').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Train:\n",
    "data = data.filter('passenger_count > 0 AND passenger_count <=15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f45afbb8090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.select('passenger_count', 'fare_amount').groupby('passenger_count').agg(F.mean('fare_amount')).sort('passenger_count')\\\n",
    ".toPandas().plot.bar('passenger_count', 'avg(fare_amount)',legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FInd whch is a taxi and whoch a van\n",
    "def taxi_or_van(df):\n",
    "    \n",
    "    '''\n",
    "        \n",
    "    Dependning on the number of seats choose if the veicle is a taxi or a van if seats > 5 than Van.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    df : Spark Database\n",
    "        a spark database containg the columns I need\n",
    "    Returns\n",
    "    -------\n",
    "    Spark Databse\n",
    "        the old databse with the new variables\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    df = df.withColumn('taxi_or_van', F.when( (col('passenger_count') <= 6), 'taxi')\\\n",
    "                      .otherwise('van'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add taxi vs van variable to df:\n",
    "\n",
    "data = taxi_or_van(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Fare Amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the statistics, it is clear that some values are not possible, I am going to remove negatives values and all values that are lower that 2.50 dollars (https://nymag.com/nymetro/urban/features/taxi/n_20286/). Hence, I will keep all values greather than 2.50 dollars. For what concerns the maximum it is difficult to think about a joutney in taxi for a price of 93963 dollars. The median instead is around 325 dollars, which is a more reasonable value. It is clear that the distribution is right skewed (mean < median) and in addition the values over the median are only 134 and looking for some of their attributes it seems that there is no real reasons to have so huge fare amount. Given that I suppose that the most of them are outlayers, therefore I am going to drop anything over the median. Furthermore, considering the high skew, it may be reasonable to drop somenthing more, too. As matter of fact, the distribution clearly shows that values over 100 dollars are very few (almost 20 thousands, the 0.03% of the total). Hence, even if some of them can be reasonable, I am going to drop anything less than 100 dollars. In a second moment it is possible to add again those values to check whether it is possible to improve the model we some special cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>54044321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>11.33294135419023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>20.86706348406584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>-300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>93963.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary        fare_amount\n",
       "0   count           54044321\n",
       "1    mean  11.33294135419023\n",
       "2  stddev  20.86706348406584\n",
       "3     min             -300.0\n",
       "4     max           93963.36"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get statistics:\n",
    "data.select('fare_amount').describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Values lower than 2.50$:\n",
    "data = data.filter('fare_amount > 2.50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.51, 375.0, 93963.36]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Quantiles:\n",
    "fair_quantiles= data.approxQuantile('fare_amount', [0.25, 0.50, 0.75], 0.25)\n",
    "fair_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs over the median: 134\n",
      "+-----------+--------------+------------------+----------+---------------+-----------+\n",
      "|fare_amount|haversine_dist|         direction|   airport|passenger_count|taxi_or_van|\n",
      "+-----------+--------------+------------------+----------+---------------+-----------+\n",
      "|      400.0|           0.0|             180.0|NO_AIRPORT|              1|       taxi|\n",
      "|      499.0|         0.012|324.26197795325294|NO_AIRPORT|              1|       taxi|\n",
      "|      475.0|         0.001|             180.0|NO_AIRPORT|              1|       taxi|\n",
      "|      450.0|         0.001| 4.396683387821781|NO_AIRPORT|              1|       taxi|\n",
      "|     377.75|        84.095| 52.57840168530231|  PICK_LGR|              1|       taxi|\n",
      "|      440.0|           0.0| 44.98473111992348|NO_AIRPORT|              1|       taxi|\n",
      "|     444.44|           0.0| 90.00000821723745|NO_AIRPORT|              1|       taxi|\n",
      "|      400.0|           0.0|               0.0|NO_AIRPORT|              1|       taxi|\n",
      "|      450.0|          0.01|309.15955313260406|NO_AIRPORT|              1|       taxi|\n",
      "|      500.0|           0.0|               0.0|NO_AIRPORT|              1|       taxi|\n",
      "+-----------+--------------+------------------+----------+---------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check some values over the median\n",
    "print('Obs over the median: {:d}'.format(data.filter('fare_amount > 375').count()))\n",
    "data.filter('fare_amount>375').select('fare_amount','haversine_dist', 'direction','airport','passenger_count','taxi_or_van').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of obs wit fare over 100$: 0.0367605463991\n"
     ]
    }
   ],
   "source": [
    "# Percentage of rows with fare amount over 100:\n",
    "print(\"Percentage of obs wit fare over 100$: {}\" .format(data.filter('fare_amount >= 100').agg(F.count('fare_amount')).toPandas().iloc[0,0] / data.count() * 100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Outlayers:\n",
    "data = data.filter('fare_amount < 100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round to the third decimal:\n",
    "data = data.withColumn('fare_amount', F.round('fare_amount', 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save DF for Tableau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Create Csv:\n"
     ]
    }
   ],
   "source": [
    "# Save new df for the dashboard:\n",
    "'''\n",
    "print('Start Create Csv:')\n",
    "s = time()\n",
    "to_csv = data.select('fare_amount','pickup_datetime','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude', 'pickup_from_center', 'dropoff_from_center',\n",
    "'haversine_dist', 'direction', 'airport', 'long_short', 'passenger_count', 'taxi_or_van','is_week_end','part_of_day')\n",
    "to_csv = to_csv.sample(fraction=0.02, seed=123)\n",
    "to_csv.coalesce(1).write.format(\"csv\").option('header', 'true').save(PATH_BUCKET+'new_train')\n",
    "print('End creation in {:.2f} minutess'.format( (time()-s)/60 ))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = data.randomSplit(weights=[0.9999, 0.0001], seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordTransformTest = Coordinates_Transform(test)\n",
    "timeTransformTest = Time_Transform(test)\n",
    "# Add Distance metrics:\n",
    "test = coordTransformTest.add_dist_metrics()\n",
    "# Add Directions:\n",
    "test = coordTransformTest.direction()\n",
    "# Add third dimension:\n",
    "test = coordTransformTest.third_dim_remap()\n",
    "# Let's Add when starting point or arrival is at the airport:\n",
    "test = coordTransformTest.airports()\n",
    "# Add Pickup and Dropoff distance from the city center:\n",
    "test = coordTransformTest.distance_from_center()\n",
    "# Add a binning for the length, based on the haversine distance:\n",
    "test = coordTransformTest.long_short_trip()\n",
    "# Get daates basic info:\n",
    "test = timeTransformTest.get_date()\n",
    "# Check if it is weekend:\n",
    "test = timeTransformTest.is_weekend()\n",
    "# Check part of the day:\n",
    "test = timeTransformTest.divide_day()\n",
    "# Transform Time to be used better in the models:\n",
    "test = timeTransformTest.add_fractions()\n",
    "test = timeTransformTest.cyclical_encoding(remove_cat=False, remove_original=True)\n",
    "# Taxi or Van\n",
    "test = taxi_or_van(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create My Base Line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Base Line I am going to use the mean value of the fare amount on the train set.\n",
    "\n",
    "The resulting RMSE = 9.40949\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 451k/451k [00:01<00:00, 265kB/s]\n",
      "Successfully submitted to New York City Taxi Fare PredictionfileName        date                 description         status    publicScore  privateScore  \n",
      "--------------  -------------------  ------------------  --------  -----------  ------------  \n",
      "submission.csv  2021-08-18 16:15:07  Average_Value_Pred  complete  9.40949      9.40949       \n",
      "submission.csv  2021-06-26 11:42:04  First Submission    complete  9.40719      9.40719       \n",
      "submission.csv  2021-06-23 15:15:57  First Submission    complete  9.40712      9.40712       \n",
      "submission.csv  2021-06-23 15:10:56  First Submission    complete  9.40712      9.40712       \n",
      "submission.csv  2021-06-23 14:58:58  None                error     None         None          \n",
      "submission.csv  2021-06-23 14:58:06  First Submission    error     None         None          \n",
      "submission.csv  2021-06-23 14:55:46  None                error     None         None          \n",
      "submission.csv  2021-06-23 14:54:33  First Submission    error     None         None          \n",
      "submission.csv  2021-06-23 14:52:36  None                error     None         None          \n",
      "submission.csv  2021-06-23 14:51:55  First Submission    error     None         None          \n",
      "submission.csv  2021-06-23 14:51:27  First Submission    error     None         None          \n",
      "submission.csv  2021-06-23 14:49:09  None                error     None         None          \n",
      "submission.csv  2021-06-23 14:47:35  First Submission    error     None         None          \n"
     ]
    }
   ],
   "source": [
    "# Compute the mean:\n",
    "mean_train = train.select(\"fare_amount\").agg({'fare_amount': 'mean'}).toPandas().iloc[0,0]\n",
    "\n",
    "# Create the submission Datase:\n",
    "original_test['fare_amount'] = mean_train\n",
    "submission = original_test[['key','fare_amount']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Submit:\n",
    "!kaggle competitions submit -c new-york-city-taxi-fare-prediction -f submission.csv -m \"Average_Value_Pred\"\n",
    "!kaggle competitions submissions -c new-york-city-taxi-fare-prediction\n",
    "\n",
    "# Remove the col fare_amount:\n",
    "original_test.drop(['fare_amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Models():\n",
    "    \n",
    "    def __init__(self, train, validation, test):\n",
    "        \n",
    "        self.train = train\n",
    "        self.valid = validation\n",
    "        self.test = test\n",
    "        \n",
    "    def save_model(self, model, model_name):\n",
    "         model.bestModel.write().overwrite().save(PATH_BUCKET + model_name)\n",
    "            \n",
    "    def load_model(self, model_name):\n",
    "        return PipelineModel.load(PATH_BUCKET + model_name)\n",
    "        \n",
    "    def linear_regression_pipeline(self, numerical_features, categorical_features, target_variable, train_perc, n_parallel, scale, model_name):\n",
    "        \n",
    "        try:\n",
    "            self.model = self.load_model(model_name)\n",
    "            \n",
    "        except:\n",
    "            # Save Model in memory:\n",
    "            self.train = self.train.cache()\n",
    "\n",
    "            # Create Indexers for each row and permorm One Hot Encoding:\n",
    "\n",
    "            indexers = [ StringIndexer(inputCol=col, outputCol=\"{0}_indexed\".format(col), handleInvalid=\"error\") \n",
    "                        for col in categorical_features ]\n",
    "\n",
    "\n",
    "            encoder = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "                                             outputCols=[\"{0}_encoded\".format(indexer.getOutputCol()) for indexer in indexers], \n",
    "                                             handleInvalid=\"error\", dropLast=False)\n",
    "\n",
    "\n",
    "            assembler = VectorAssembler(inputCols=encoder.getOutputCols() + numerical_features, outputCol=\"features\")\n",
    "\n",
    "            # Scale the Dataset:\n",
    "            if scale:\n",
    "                scaler = StandardScaler(inputCol=assembler.getOutputCol(), outputCol=\"std_\"+assembler.getOutputCol(), \n",
    "                                        withStd=True, withMean=True)\n",
    "\n",
    "\n",
    "            # Prepare the linear Regression:\n",
    "\n",
    "            if target_variable != \"label\":\n",
    "                self.train = self.train.withColumnRenamed(target_variable, \"label\")\n",
    "\n",
    "\n",
    "            if scale:\n",
    "                lr = LinearRegression(featuresCol=\"std_features\", labelCol=\"label\", fitIntercept=True)\n",
    "                pipeline = Pipeline(stages=indexers + [encoder] + [assembler] + [scaler] + [lr]) \n",
    "            else:\n",
    "                lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\", fitIntercept=True)\n",
    "                pipeline = Pipeline(stages=indexers + [encoder] + [assembler] + [lr])\n",
    "\n",
    "\n",
    "            # HyperParamters Tuning:\n",
    "            param_grid = ParamGridBuilder()\\\n",
    "            .addGrid(lr.regParam, [0.1, 0.001, 0.0001]) \\\n",
    "            .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "            .build()\n",
    "\n",
    "            tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                                       estimatorParamMaps=param_grid,\n",
    "                                       evaluator=RegressionEvaluator(metricName=\"rmse\"),\n",
    "                                       trainRatio=train_perc,\n",
    "                                       parallelism=n_parallel,\n",
    "                                       seed=123)\n",
    "\n",
    "\n",
    "            # Run the model:\n",
    "            self.model = tvs.fit(self.train)\n",
    "            self.save_model(self.model, model_name)\n",
    "\n",
    "            # Compute and save metrics:\n",
    "            self.compute_metrics(scale, model_name)\n",
    "\n",
    "        return self.model\n",
    "    \n",
    "    def compute_adj_r2(self, r2, scale, is_train):\n",
    "        \n",
    "        if is_train:\n",
    "            if scale:\n",
    "                n_features = self.model.transform(self.train).select('std_features').first()[0].size\n",
    "                n_obs = self.train.count()\n",
    "                adj_r2 = (1 - (1 - r2) * ((n_obs - 1) / (n_obs - n_features - 1)))\n",
    "            else:\n",
    "                n_features = self.model.transform(self.train).select('features').first()[0].size\n",
    "                n_obs = self.train.count()\n",
    "                adj_r2 = (1 - (1 - r2) * ((n_obs - 1) / (n_obs - n_features - 1)))\n",
    "        else:\n",
    "            if scale:\n",
    "                n_features = self.model.transform(self.valid).select('std_features').first()[0].size\n",
    "                n_obs = self.valid.count()\n",
    "                adj_r2 = (1 - (1 - r2) * ((n_obs - 1) / (n_obs - n_features - 1)))\n",
    "            else:\n",
    "                n_features = self.model.transform(self.valid).select('features').first()[0].size\n",
    "                n_obs = self.valid.count()\n",
    "                adj_r2 = (1 - (1 - r2) * ((n_obs - 1) / (n_obs - n_features - 1)))\n",
    "            \n",
    "            \n",
    "        return adj_r2\n",
    "    \n",
    "    def evaluate_model(self, predictions, metric):\n",
    "    \n",
    "        evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "                                        predictionCol=\"prediction\",\n",
    "                                        metricName=metric)\n",
    "\n",
    "        return evaluator.evaluate(predictions)\n",
    "    \n",
    "    def compute_metrics(self, scale, model_name):\n",
    "        \n",
    "        # Get summaries\n",
    "        training_result = self.model.bestModel.stages[-1].summary\n",
    "        \n",
    "        # Get p values:\n",
    "        p_values = training_result.pValues\n",
    "        std_err = training_result.coefficientStandardErrors\n",
    "        t_val = training_result.tValues\n",
    "        \n",
    "        # Compute Train Metrics:\n",
    "        rmse_train = training_result.rootMeanSquaredError\n",
    "        r2_train = training_result.r2\n",
    "        adj_r2_train = self.compute_adj_r2(r2_train,scale=scale,is_train=True)\n",
    "        # Unpersist the train:\n",
    "        self.train = self.train.unpersist()\n",
    "        \n",
    "        # Cache Validation:\n",
    "        self.valid = self.valid.cache()\n",
    "        # Compute Validation Metrics:\n",
    "        self.valid = self.valid.withColumnRenamed(\"fare_amount\", \"label\")\n",
    "        predictions = self.model.transform(self.valid)\n",
    "        \n",
    "        rmse_valid = self.evaluate_model(predictions, 'rmse')\n",
    "        r2_valid = self.evaluate_model(predictions, 'r2')\n",
    "        adj_r2_valid = self.compute_adj_r2(r2_valid, scale=scale, is_train=False)\n",
    "        # Unpersist the valid:\n",
    "        self.valid = self.valid.unpersist()\n",
    "        \n",
    "        # Save Metrics in A dcit:\n",
    "        metrics = {\n",
    "            'train':{\n",
    "                'rmse': rmse_train,\n",
    "                'R2': r2_train,\n",
    "                'adj_R2': adj_r2_train     \n",
    "            },\n",
    "            'valid': {\n",
    "                'rmse': rmse_valid,\n",
    "                'R2': r2_valid,\n",
    "                'adj_R2': adj_r2_valid\n",
    "            },\n",
    "            'coeff':{\n",
    "                'std_err': std_err,\n",
    "                't_val': t_val,\n",
    "                'p_val': p_values\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save Metrics:\n",
    "        with open(PATH_MAIN +  model_name + '.json', 'w') as f:\n",
    "            json.dump(metrics, f)\n",
    "        Bucket.upload_file_to_bucket(model_name +'.json', PATH_MAIN, 'metrics/')\n",
    "        \n",
    "        \n",
    "    \n",
    "    def training_results(self, model_name, model_type):\n",
    "        \n",
    "        if model_type == 'linear':\n",
    "            \n",
    "            # Load Model and Json:\n",
    "            regressor = self.load_model(model_name)\n",
    "            Bucket.get_file_from_bucket(model_name + '.json', PATH_MAIN, 'metrics/')\n",
    "            file = open(PATH_MAIN + model_name + '.json', 'r')\n",
    "            metrics = json.load(file)\n",
    "            file.close()\n",
    "            \n",
    "            # Print HyperParams:\n",
    "            print(\"***** Hyper Parameters *****\")\n",
    "            print('regParam: ' + re.search(r'(?<=\\w:\\s)(\\d+.\\d+)', regressor.stages[-1].explainParam('regParam')).group(0))\n",
    "            print('elasticNetParam: ' + re.search(r'(?<=\\w:\\s)(\\d+.\\d+)', regressor.stages[-1].explainParam('elasticNetParam')).group(0))\n",
    "            print(\"***** Hyper Parameters *****\\n\")\n",
    "            \n",
    "            # Print coeffcients:\n",
    "            intercept = regressor.stages[-1].intercept\n",
    "            coeff = [intercept] + regressor.stages[-1].coefficients\n",
    "            coeff_names = ['Intercept'] + regressor.stages[1].getInputCols()\n",
    "            coeff_metr = metrics['coeff']\n",
    "            \n",
    "            print(\"***** Parameters *****\")\n",
    "            print (\"Feature        |  Estimate   |   Std.Error | t Values  |  P-value\")\n",
    "            for i in range(len(coeff)):\n",
    "                print ('{}: '.format(coeff_names[i]),\\\n",
    "                       '{:10.6f}'.format(coeff[i]),\\\n",
    "                       '{:10.6f}'.format(coeff_metr['std_err'][i]),\\\n",
    "                       '{:8.3f}'.format(coeff_metr['t_val'][i]),\\\n",
    "                       '{:e}'.format(coeff_metr['p_val'][i]))\n",
    "            print(\"***** Parameters *****\\n\")\n",
    "\n",
    "            # Print Metrics:\n",
    "            print(\"***** Training Metrics *****\")\n",
    "            for k, v in metrics['train'].items():\n",
    "                print(\"{}: {:.3f}\".format(str(k), v))\n",
    "            print(\"***** Training Metrics *****\\n\")\n",
    "\n",
    "            print(\"***** Validation Metrics *****\")\n",
    "            for k, v in metrics['valid'].items():\n",
    "                print(\"{}: {:.3f}\".format(str(k), v))\n",
    "            print(\"***** Validation Metrics *****\\n\")\n",
    "            \n",
    "            \n",
    "    def prepare_submission(self, model_name=''):\n",
    "        \n",
    "        # Make the predictions:\n",
    "        if not model_name:\n",
    "            predictions = self.model.transform(self.test).select('prediction').withColumnRenamed('prediction','fare_amount').toPandas()\n",
    "        else:\n",
    "            regressor = self.load_model(model_name)\n",
    "            predictions = regressor.transform(self.test).select('prediction').withColumnRenamed('prediction','fare_amount').toPandas()\n",
    "            \n",
    "        # Prepare the Submission:\n",
    "        submission = pd.concat([original_test['key'], predictions['fare_amount']], axis=1)\n",
    "        submission.to_csv('submission.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Linear Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyModel = Models(train, validation, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL = ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude', 'passenger_count']\n",
    "CATEGORICAL = []\n",
    "TARGET = 'fare_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the model:\n",
    "MyModel.linear_regression_pipeline(NUMERICAL, CATEGORICAL, TARGET, train_perc=0.90, n_parallel=2, scale=True,\n",
    "                                   model_name='linear_basic_features')\n",
    "# Print Anything about the model:\n",
    "MyModel.training_results(model_name='linear_basic_features', model_type='linear')\n",
    "\n",
    "# Prepare the submission:\n",
    "MyModel.prepare_submission()\n",
    "\n",
    "# Submit:\n",
    "!kaggle competitions submit -c new-york-city-taxi-fare-prediction -f submission.csv -m \"Linear_Basic_Features\"\n",
    "!kaggle competitions submissions -c new-york-city-taxi-fare-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL = ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude', 'passenger_count',\n",
    "             'abs_dist_longitude', 'abs_dist_latitude', 'manhattan_dist', 'euclidean_dist', 'haversine_dist', \n",
    "             'direction', 'pickup_x','pickup_y', 'pickup_z','dropoff_x', 'dropoff_y','dropoff_z', 'pickup_from_center',\n",
    "             'dropoff_from_center', 'month_sin', 'month_cos','week_of_year_sin','week_of_year_cos','quarter_sin',\n",
    "             'quarter_cos','day_of_month_sin', 'day_of_month_cos', 'day_of_week_sin', 'day_of_week_cos', 'day_of_yearh_sin',\n",
    "             'day_of_year_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'year'\n",
    "            ]\n",
    "CATEGORICAL = ['airport', 'long_short', 'is_week_end', 'part_of_day', 'taxi_or_van']\n",
    "TARGET = 'fare_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|fare_amount|        prediction|\n",
      "+-----------+------------------+\n",
      "|        4.2| 12.04217940355572|\n",
      "|        4.5| 11.12128165470414|\n",
      "|        5.3| 8.449449157563162|\n",
      "|        5.7| 7.573554991075827|\n",
      "|        6.1| 9.128659752618217|\n",
      "|        6.5| 6.698902737927666|\n",
      "|        6.9|10.733033651486718|\n",
      "|        6.9| 9.953308949138929|\n",
      "|        7.0|10.346287677954493|\n",
      "|        7.8| 8.137613717106127|\n",
      "|        8.5|18.148985228112238|\n",
      "|        8.9|10.357331084261542|\n",
      "|        9.0| 9.302898200903002|\n",
      "|        9.0|11.576563128585061|\n",
      "|        9.5|10.754012136803404|\n",
      "|       10.1|  9.92052505873082|\n",
      "|       10.9| 9.666246348060744|\n",
      "|       11.0| 10.96939863561721|\n",
      "|       16.0|  8.53342723567898|\n",
      "|       19.0|10.510276760217407|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MyModel.model.transform(validation).select('fare_amount', 'prediction').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
