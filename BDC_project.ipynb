{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pyspark_dist_explore\n",
    "!pip install chart-studio\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work With Files\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "# Useful libraries:\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# To Plot:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Pyspark Lib:\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark_dist_explore import hist\n",
    "\n",
    "# Preprocess:\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# Pysparl ML:\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_BUCKET = 'gs://nyc_comp_bk/'\n",
    "PATH_DATA = '/home/ubuntu/NYC_Taxi/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/ubuntu/NYC_Taxi/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Work_On_Bucket():\n",
    "    \n",
    "    def __init__(self, bucket_name):\n",
    "        # Get access to the bucket:\n",
    "        storage_client = storage.Client()\n",
    "        self.bucket = storage_client.get_bucket(bucket_name)\n",
    "        \n",
    "    def get_file_from_bucket(self, file_name, save_path):\n",
    "        # Download the file:\n",
    "        blob = self.bucket.blob(file_name)\n",
    "        blob.download_to_filename(''.join([save_path, file_name]))\n",
    "            \n",
    "    def upload_file_to_bucket(self, file_name, folder_path):\n",
    "        # Upload the File\n",
    "        object_to_save = self.bucket.blob(file_name)\n",
    "        object_to_save.upload_from_filename(folder_path + file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bucket = Work_On_Bucket('nyc_comp_bk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set kaggle:\n",
    "! mkdir ~/.kaggle\n",
    "Bucket.get_file_from_bucket('kaggle.json', '/home/ubuntu/NYC_Taxi/')\n",
    "! cp /home/ubuntu/NYC_Taxi/kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download The Dataset\n",
    "!kaggle competitions download -c new-york-city-taxi-fare-prediction\n",
    "\n",
    "# Unzip the Files\n",
    "! unzip new-york-city-taxi-fare-prediction.zip -d /home/ubuntu/NYC_Taxi/data/\n",
    "! rm new-york-city-taxi-fare-prediction.zip\n",
    "\n",
    "# Upload databses to bucket:\n",
    "print('Start Uploding!')\n",
    "Bucket.upload_file_to_bucket('train.csv', PATH_DATA)\n",
    "Bucket.upload_file_to_bucket('test.csv', PATH_DATA)\n",
    "print('Succesfully Uploaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Steps (Load + Checks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data:\n",
    "train = spark.read.load(PATH_BUCKET+\"train.csv\", format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "test = spark.read.load(PATH_BUCKET+\"test.csv\", format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# Load Test (Because pyspark changes the timestamp):\n",
    "Bucket.get_file_from_bucket('test.csv', '')\n",
    "original_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DB shape:\n",
    "ncol = len(train.columns)\n",
    "nrow = train.count()\n",
    "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(nrow, ncol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the schema:\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some basic Statistics:\n",
    "stats = train.select(train.columns[1:]).describe()\n",
    "stats.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Nulls:\n",
    "for c in train.columns[2:]:\n",
    "    nans = train.where(col(c).isNull()).count()\n",
    "    print('{:s}: {:d}'.format(c, nans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Rows with Missing Values:\n",
    "train = train.na.drop(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Duplicates:\n",
    "print('The Duplicates are: {:d}'.format(train.count()-train.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates:\n",
    "train = train.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work With the Fare Price (Dependent Variale):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the statistics, it is clear that some values are not possible, I am going to remove negatives values and all values that are lower that 2.50 dollars (https://nymag.com/nymetro/urban/features/taxi/n_20286/). Hence, I will keep all values greather than 2.50 dollars. By taking into account the max value, it is clear that some outlayers are present, nobady would want to spend 93,963 dollars for a trip. Working with the quantiles it is clear that there is somenthing wrong with the fare amount, the median is 999.9 dollars. As starting point let's remove what I am pretty sure that is an outlayer, namely everything over the fourth quntile, which represents all values greater than 93963.36 dollars. Computing again the quantiles now the fourth quantile is 999.99 dollars, Checking the other variables associated with these high fare amounts, it comes up that some of them doesn't make any sense: no changes in lat and long pick up and drop off, too small journey with one or two people for a so high price, missing values for lat and long and so on. I am going to keep al observations with fear amount less than 999.99 dollars, hopping that with the EDA I can clean more the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics:\n",
    "train.select('fare_amount').describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Values lower than 2.50$:\n",
    "train = train.filter('fare_amount > 2.50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Quantiles:\n",
    "train.approxQuantile('fare_amount', [0.25, 0.50, 0.75, 0.975], 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Outlayers:\n",
    "train = train.filter('fare_amount < 93963.36')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Again Quantiles\n",
    "train.approxQuantile('fare_amount', [0.25, 0.50, 0.75, 0.975], 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove other Outlayers:\n",
    "train = train.filter('fare_amount < 999.99')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Quantiles:\n",
    "train.approxQuantile('fare_amount', [0.25, 0.50, 0.75, 0.975], 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create My Base Line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Base Line I am going to use a Multiple Linear Regression that takes as input all the scaled (mean=0, sd=1) numerical variables.\n",
    "As result I get an RMSE = 9.40719 on the Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL = ['pickup_longitude',\n",
    "            'pickup_latitude',\n",
    "            'dropoff_longitude',\n",
    "            'dropoff_latitude',\n",
    "            'passenger_count']\n",
    "TARGET = 'fare_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature Vector:\n",
    "assembler = VectorAssembler(inputCols=NUMERICAL, outputCol=\"features\")\n",
    "train_df = assembler.transform(train)\n",
    "train_df = train_df.select('features', TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Data:\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"ScaledFeatures\")\n",
    "scalerModel = scaler.fit(train_df)\n",
    "train_df = scalerModel.transform(train_df)\n",
    "train_df = train_df.select('ScaledFeatures', TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Linear Regression:\n",
    "lr = LinearRegression(featuresCol=\"ScaledFeatures\", labelCol=TARGET, maxIter=10)\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Print Stats:\n",
    "training_result = lr_model.summary\n",
    "print(\"***** Training Set *****\")\n",
    "print(\"RMSE: {:.3f}\".format(training_result.rootMeanSquaredError))\n",
    "print(\"MAE: {:.3f}\".format(training_result.meanAbsoluteError))\n",
    "print(\"R2: {:.3f}\".format(training_result.r2))\n",
    "print(\"***** Training Set *****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test:\n",
    "test_df = assembler.transform(test.select(NUMERICAL))\n",
    "test_df = test_df.select('features')\n",
    "test_df = scalerModel.transform(test_df)\n",
    "\n",
    "# Make Predictions:\n",
    "predictions = lr_model.transform(test_df).select('prediction').withColumnRenamed('prediction','fare_amount').toPandas()\n",
    "\n",
    "# Prepare the Submission:\n",
    "submission = pd.concat([original_test['key'], predictions['fare_amount']], axis=1)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Submit:\n",
    "!kaggle competitions submit -c new-york-city-taxi-fare-prediction -f submission.csv -m \"First Submission\"\n",
    "!kaggle competitions submissions -c new-york-city-taxi-fare-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Latitude and Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the study of the quantiles it is clear that is plenty of outlayers, because the NYC coordinates are (lat=40.730610 lon=-73.935242). Even working with quantiles, removing anything out of the I and III quartile things don't change so much. Therefore, I opt for anothe option, I draw a triangle around the area of interest, a triangle because NY City is on an icleand which reminds a triangle, but the area is bigger than the iceland, to take into acocunt possible trips from the countriside. Then I remove all observations for which the pick up position or drop off position outside the area.\n",
    "To decide whether a point is in the triangle I walk clockwise or counterclockwise around the triangle and project the point onto the segment I am crossing by using the dot product. Finally, check that the vector created is on the same side for each of the triangle's segments.\n",
    "\n",
    "The it is possible to compute new features that can be useful both for the model and to remove more outlayers, namely the Absolute Distance, the Haversinee Distance and the Minkowski Distance (that for p=1 is the Manhattan and for p=2 is the Euclidean distance). All the distances are expressed in kilometers. To obtain some of them I need to know to how much km corresponds the variation of 1 degree for each coordinates and it turns put that for Latitude it is 111 Km and for Longitude is 85 Km. Checking the statistics of th emetrics it is clear that there are many outlayers. To drop them I am going to use a kind of voting, for each observation I get the mean of manhattan, euclidean and haversine distances metrics and remove the observation if its distance mean is less than 0.5 (namel less than 500m trip).\n",
    "\n",
    "Exploiting the function to compute the haversian distance (which is the more realistic) I compute the distanes from the airports, both for pick up and drop off and I create a column that says whether the pick up or dorp off happened near by the airport. It is possible that there the costs are higher or maybe fixed. It is possible to check using a groupby that the average price is far away higher than the one for a standard trip. Furthermore, for rhe most of them the standard deviation shows that maybe the prices are not fixed or anyway that the prices can change up to +/-30$ for the most volatile airport (EWR).\n",
    "\n",
    "Then I suppose that a there must be a relationship between fear amount and length of the trip (it maybe also connected with duration). Hence, I create a variable tha distinguish among trips under the mean and over the mean. It seems that my hypothesis is confirmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check I, II and III Quantiles:\n",
    "np.array([train.approxQuantile('pickup_longitude', [0.25, 0.50, 0.75], 0.25),\n",
    "          train.approxQuantile('pickup_latitude', [0.25, 0.50, 0.75], 0.25),\n",
    "          train.approxQuantile('dropoff_longitude', [0.25, 0.50, 0.75], 0.25),\n",
    "          train.approxQuantile('dropoff_latitude', [0.25, 0.50, 0.75], 0.25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write function to check if a point is a triangle:\n",
    "\n",
    "def point_in_triangle(lat, long):\n",
    "    \"\"\"\n",
    "    phuclv (https://stackoverflow.com/questions/2049582/how-to-determine-if-a-point-is-in-a-2d-triangle)\n",
    "    Returns True if the point is inside the triangle\n",
    "    and returns False if it falls outside.\n",
    "    - The argument *point* is a tuple with two elements\n",
    "    containing the X,Y coordinates respectively.\n",
    "    - The argument *triangle* is a tuple with three elements each\n",
    "    element consisting of a tuple of X,Y coordinates.\n",
    "    \"\"\"\n",
    "    # Unpack arguments\n",
    "    x = lat\n",
    "    y = long\n",
    "    ax, ay = (39.74274655286439, -75.26399018397262)\n",
    "    bx, by = (41.58840050439113, -75.18708588709761)\n",
    "    cx, cy = (41.07279488244855, -71.76484467616011)\n",
    "    # Segment A to B\n",
    "    side_1 = (x - bx) * (ay - by) - (ax - bx) * (y - by)\n",
    "    # Segment B to C\n",
    "    side_2 = (x - cx) * (by - cy) - (bx - cx) * (y - cy)\n",
    "    # Segment C to A\n",
    "    side_3 = (x - ax) * (cy - ay) - (cx - ax) * (y - ay)\n",
    "    # All the signs must be positive or all negative\n",
    "    return (side_1 < 0.0) == (side_2 < 0.0) == (side_3 < 0.0)\n",
    "\n",
    "def is_in_area(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    pick = point_in_triangle(lat1, lon1)\n",
    "    drop = point_in_triangle(lat2, lon2)\n",
    "    \n",
    "    return all([pick, drop])\n",
    "\n",
    "is_in_area_udf = F.udf(is_in_area, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any point outside the given area:\n",
    "train = train.withColumn('keep', is_in_area_udf( col('pickup_latitude'), col('pickup_longitude'), col('dropoff_latitude'), col('dropoff_longitude') ) )\n",
    "train = train.filter(\"keep == True\")\n",
    "train = train.drop('keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select(col('pickup_latitude'), col('pickup_longitude'), col('dropoff_latitude'), col('dropoff_longitude')).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Functions needed to Create Coordinates Features\n",
    "\n",
    "class Coordinates_Transform():\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.df = df\n",
    "        self.cols = df.columns\n",
    "        self.haversine_udf = F.udf(Coordinates_Transform.haversine_stat, DoubleType())\n",
    "        self.direction_udf = F.udf(Coordinates_Transform.calculate_dir, DoubleType())\n",
    "        self.airports_udf = F.udf(Coordinates_Transform.identify_airports, StringType())\n",
    "\n",
    "    @staticmethod\n",
    "    def haversine_stat(pick_lat, drop_lat, pick_long, drop_long):\n",
    "\n",
    "        longit_a, latit_a, longit_b, latit_b = map(math.radians, [pick_long,  pick_lat, drop_long, drop_lat])\n",
    "        dist_longit = longit_b - longit_a\n",
    "        dist_latit = latit_b - latit_a\n",
    "        # Calculate area\n",
    "        area = math.sin(dist_latit/2)**2 + math.cos(latit_a) * math.cos(latit_b) * math.sin(dist_longit/2)**2\n",
    "        # Calculate the central angle\n",
    "        central_angle = 2 * math.asin(math.sqrt(area))\n",
    "        RADIUS = 6371\n",
    "        # Calculate Distance\n",
    "        distance = central_angle * RADIUS\n",
    "        return round(distance, 3)\n",
    "\n",
    "    def add_dist_metrics(self):\n",
    "\n",
    "        # Absolute Lat e Long Distance:\n",
    "        self.df = self.df.withColumn( 'abs_dist_longitude', F.round(F.abs( col('pickup_longitude') - col('dropoff_longitude') ) * 85, 3 ) )\n",
    "        self.df = self.df.withColumn( 'abs_dist_latitude', F.round( F.abs( col('pickup_latitude') - col('dropoff_latitude') ) * 111 , 3 ) )\n",
    "\n",
    "        # Manhattan Distance:\n",
    "        self.df = self.df.withColumn( 'manhattan_dist', F.round( col('abs_dist_longitude') + col('abs_dist_latitude'), 3 ) )\n",
    "\n",
    "        # Euclidean Distance:\n",
    "        self.df = self.df.withColumn( 'euclidean_dist',  F.round( F.sqrt( col('abs_dist_longitude')**2 + col('abs_dist_latitude')**2 ), 3 ) )\n",
    "\n",
    "        # Haversine Distance:\n",
    "        self.df = self.df.withColumn( 'haversine_dist', \n",
    "                                     self.haversine_udf( col('pickup_latitude'), col('dropoff_latitude'), col('pickup_longitude'), col('dropoff_longitude') ) ) \n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_dir(lat1, lon1, lat2, lon2, euclid):\n",
    "        # To improve cmputational time, use the df column ad hoc\n",
    "        # I cannot do it because I have compute the abs\n",
    "\n",
    "        delta_lon = lon2 - lon1\n",
    "        delta_lat = lat2 - lat1\n",
    "        \n",
    "        return math.atan2( delta_lat, delta_long) * 180/math.pi\n",
    "        '''\n",
    "        if delta_lon > 0:\n",
    "            return (180/math.pi) * math.asin( delta_lat/(euclid+0.001) )\n",
    "        elif delta_lon < 0 and delta_lat > 0:\n",
    "            return 180 - (180/math.pi) * math.asin( delta_lat/(euclid+0.001) )\n",
    "        elif delta_lon < 0 and delta_lat < 0:\n",
    "            return -180 - (180/math.pi) * math.asin( delta_lat/(euclid+0.001) )\n",
    "        '''\n",
    "    def direction(self):\n",
    "        self.df = self.df.withColumn('direction', self.direction_udf( col('pickup_latitude'), col('pickup_longitude'), col('dropoff_latitude'), \n",
    "                                                                     col('dropoff_longitude'), col('euclidean_dist') ))\n",
    "        return self.df\n",
    "\n",
    "    @staticmethod\n",
    "    def identify_airports(pick_lat, drop_lat, pick_long, drop_long):\n",
    "\n",
    "        # Set the coordinates of airports:\n",
    "        JFK_LAT = 40.641766\n",
    "        JFK_LON = -73.780968\n",
    "\n",
    "        LGR_LAT = 40.773013\n",
    "        LGR_LON = -73.870229\n",
    "\n",
    "        EWR_LAT = 40.689531\n",
    "        EWR_LON = -74.174462\n",
    "\n",
    "        # Compute distances:\n",
    "        pick_up_jfk = Coordinates_Transform.haversine_stat(JFK_LAT, pick_lat, JFK_LON, pick_long)\n",
    "        drop_off_jfk = Coordinates_Transform.haversine_stat(drop_lat, JFK_LAT, drop_long, JFK_LON)\n",
    "\n",
    "        pick_up_lgr = Coordinates_Transform.haversine_stat(LGR_LAT, pick_lat, LGR_LON, pick_long)\n",
    "        drop_off_lgr = Coordinates_Transform.haversine_stat(drop_lat, LGR_LAT, drop_long, LGR_LON)\n",
    "\n",
    "        pick_up_ewr = Coordinates_Transform.haversine_stat(EWR_LAT, pick_lat, EWR_LON, pick_long)\n",
    "        drop_off_ewr = Coordinates_Transform.haversine_stat(drop_lat, EWR_LAT, drop_long, EWR_LON)\n",
    "\n",
    "        # Assign a value:\n",
    "        if pick_up_jfk < 1:\n",
    "            return 'PICK_JFK'\n",
    "        elif drop_off_jfk < 1:\n",
    "            return 'DROP_JFK'\n",
    "        if pick_up_lgr < 1:\n",
    "            return 'PICK_LGR'\n",
    "        elif drop_off_lgr < 1:\n",
    "            return 'DROP_LGR'\n",
    "        if pick_up_ewr < 1:\n",
    "            return 'PICK_EWR'\n",
    "        elif drop_off_ewr < 1:\n",
    "            return 'DROP_EWR'\n",
    "        else:\n",
    "            return 'NO_AIRPORT'\n",
    "  \n",
    "    def airports(self):\n",
    "\n",
    "        self.df = self.df.withColumn('airport', self.airports_udf( col('pickup_latitude'), col('dropoff_latitude'), col('pickup_longitude'), col('dropoff_longitude') ))\n",
    "        return self.df\n",
    "    \n",
    "    def long_short_trip(self):\n",
    "        \n",
    "        mean_val = self.df.agg(F.avg(col('haversine_dist'))).collect()[0][0]\n",
    "        self.df = self.df.withColumn( 'long_short', F.when(col('haversine_dist') <= mean_val, 'SHORT').otherwise('LONG') )\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Distance metrics:\n",
    "coordTransform = Coordinates_Transform(train)\n",
    "train = coordTransform.add_dist_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Check df:\n",
    "train.select('abs_dist_longitude', 'abs_dist_latitude', 'manhattan_dist', 'euclidean_dist', 'haversine_dist').describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove outlayers:\n",
    "DIST_METRICS = ['manhattan_dist', 'euclidean_dist', 'haversine_dist']\n",
    "n = len(DIST_METRICS)\n",
    "row_mean  = (sum(col(x) for x in DIST_METRICS) / n).alias(\"mean_dist\")\n",
    "train = train.where( (row_mean >= 0.5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select('abs_dist_longitude', 'abs_dist_latitude', 'manhattan_dist', 'euclidean_dist', 'haversine_dist').describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Directions:\n",
    "train = coordTransform.direction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hist(ax, train.select('direction'), bins = 180)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Add when starting point or arrival is at the airport:\n",
    "train = coordTransform.airports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check avg price for the airports:\n",
    "train.select('airport','fare_amount').groupBy('airport').agg(F.mean('fare_amount'), F.stddev('fare_amount'), F.count('fare_amount')).sort('avg(fare_amount)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a binning for the length, based on the haversine distance:\n",
    "train = coordTransform.long_short_trip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check avg price for binned trip length:\n",
    "train.select('long_short' ,'fare_amount').groupBy('long_short').agg(F.mean('fare_amount'), F.stddev('fare_amount'), F.count('fare_amount')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
